---
title: "Sintak analisis TPG UAS"
author: "Raziqizzan Putrandi"
date: "2023-12-05"
output:
   html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: false
---

# Pertemuan 8 Analisis Faktor

## Input data

Data yang digunakan merupakan data pelamar kerja, di mana seorang HRD ingin mengindentifikasi faktor-faktor yang dapat menjelaskan 12 peubah yang telah dikumpulkan oleh departemen mereka untuk mengukur setiap pelamar kerja. Pegawai HRD menilai pelamar kerja dengan menggunakan skala 1 (rendah) sampai 10 (tinggi). Mereka menggumpulkan penilaian untuk 50 pelamar kerja.

```{r}
library(psych)
library(ggcorrplot)
data_jobApp <- read.csv("https://raw.githubusercontent.com/nurkhamidah/dat/main/job-applicants.csv", sep = ";")
head(data_jobApp)
```

## Tahap 1 - Eksplorasi Data dengan Melihat Korelasi Antar Peubah

Tujuan utama melihat korelasi data dalam analisis faktor adalah untuk memahami pola hubungan antara variabel-variabel dan memastikan bahwa data yang digunakan dalam analisis faktor sesuai dan relevan. Hal ini akan membantu dalam pemilihan variabel, penentuan jumlah faktor, dan interpretasi hasil analisis faktor.

```{r}
cor_jobApp <- cor(data_jobApp)
ggcorrplot(cor_jobApp, type = "lower", lab = TRUE)
```

Jika  diperhatikan hasil korelasi tersebut akan diperoleh beberapa kelompok peubah yang memiliki korelasi yang besar dalam kelompok peubah tersebut namun korelasinya dengan peubah diluar kelompok tersebut kecil. Sebagai ilustrasi, peubah **_Potential_** dengan **_Academic record_**, **_Experience_**, dan **_Job Fit_** memiliki korelasi yang besar dibandingkan dengan peubah lainnya. Peubah-peubah dengan korelasi tinggi ini bisa diukur dengan baik oleh suatu peubah laten yang disebut **_factor_**.

## Tahap 2 - Menentukan Banyaknya Factor

Analisis Faktor di R dapat dilakukan dengan menggunakan fungsi `fa()` dari _package_ `psych`. Fungsi `fa()` memiliki argumen `fm` yang menyatakan metode pendugaaan dan `rotate` yang menyatakan jenis rotasi yang digunakan. Argumen `nfactor` menyatakan banyaknya factornya.

Daftar yang bisa diisi dalam argumen `fm` adalah sebagai berikut:

| isi argumen | kegunaan |
| ----------- | -------- |
| `minres`    | komponen utama (_default_) |
| `ols`       | kuadrat terkecil |
| `wls`       | kuadrat terkecil terboboti |
| `gls`       | kuadrat terkecil terampat  |
| `pa`        | metode faktor utama |
| `ml`        | metode kemungkinan maksimum |
| `alpha`     | analisis faktor alpha |

Daftar yang bisa diisi dalam argumen `rotate` adalah sebagai berikut:

| isi argumen | kegunaan |
| ----------- | -------- |
| `oblimin` | direct oblimin (_default_) |
| `none`    | tanpa rotasi |
| `varimax` | varimax |
| `quartimax` | quartimax |
| `equamax` | equamax  |
| `promax` | promax/oblique |

Untuk kasus data pelamar kerja ini, kita akan gunakan metode pendugaan komponen utama. Banyaknya factor yang akan diduga kita set sebanyak peubah yang ada di data pelamar kerja ini.

| Note: |
| ---------- |
| Metode pendugaan komponen utama secara default menggunakan semua peubah yang ada di data asli. Namun, pada fungsi `fa()` pengguna harus menentukan terlebih dahulu jumlah factor-nya. Aplikasi lain seperti SAS SPSS dan Minitab tidak perlu menginputkan jumlah factor-nya. |

### 1. Penentuan banyak factor menggunakan **_proportion of the sample variance explained_**

```{r}
fa_jobApp <- fa(data_jobApp, nfactors = 12, fm = "minres", rotate = "none")
fa_jobApp$Vaccounted
```

**MR1** sampai **MR12** merupakan nama dari faktor-faktor yang telah diekstrasksi. Kemudian, **SS loadings** merupakan keragamaan dari masing-masing factor.

Jika dilihat pada **_Cumulative Proportion_**, dengan menggunakan 2 factor saja sudah bisa menjelaskan lebih dari 71% keragamaan dari data asal. Namun, penambahan **_Cumulative Proportion_** dari penggunaan faktor 2 sampai faktor 4 masih cukup besar sehingga masih masuk akal untuk menggunakan 4 factor.

### 2. Menggunakan **_scree plot_**

```{r}
SS_loadings <- fa_jobApp$Vaccounted[1,]
number_of_factor <- seq_along(SS_loadings)
plot(number_of_factor, SS_loadings, type = "b", main = "Scree Plot", pch = 16)
```

Berdasarkan _scree plot_, pola garis yang berbentuk seperti siku tangan berada pada factor kedua. Sehingga menurut _scree plot_ cukup 2 factor saja yang digunakan.

Karena menurut _scree plot_ dan _proportion of variance explained_ berbeda, akan dicoba keduanya dan melihat factor mana yang lebih mudah diinterpretasikan.

## Tahap 3 - Estimasi Factor Loading (Menggunakan 4 Factor)

```{r}
fa_jobApp4 <- fa(data_jobApp, nfactors = 4, fm = "minres", rotate = "none")
print(fa_jobApp4$loadings, cut = 0)
```

Jika kita perhatikan, nilai factor loading yang besar berkumpul di peubah factor yang **pertama**, sehingga akan sulit untuk menginterpretasikan factor 2 sampai factor 4. Oleh karena itu, kita bisa melakukan rotasi faktor agar lebih mudah menginterpretasikanya.

| Note: |
| ---------- |
| Argumen `cut=0` berarti kita menampilkan semua factor loading, jika kita tidak tentukan isi dari argumen `cut`, secara default R tidak menampilkan nilai yang berkisar antara [-0.1, 0.1] |

## Tahap 3 - Estimasi Factor Loading (Menggunakan 2 Factor)

```{r}
fa_jobApp2 <- fa(data_jobApp, nfactors = 2, fm = "minres", rotate = "none")
print(fa_jobApp2$loadings, cut = 0)
```

Untuk dua faktor pun, nilai factor loading yang besar terkumpul pada faktor pertama sehingga akan sulit diinterpretasikan. Oleh karena itu, kita akan melakukan rotasi faktor untuk penggunaan dua factor ini.

| Note: |
| ---------- |
| Ukuran besar atau kecil dari factor loading itu relatif. Beberapa buku menyebutkan nilai diatas 0.6 sudah besar. |

## Tahap 4 - Rotasi Factor (Menggunakan 4 Factor)

```{r}
fa_jobApp4_rotate <- fa(data_jobApp, nfactors = 4, fm = "minres", rotate = "varimax")
print(fa_jobApp4_rotate$loadings, cut = 0)
```

Setelah dirotasi menggunakan metode **varimax**, terlihat bahwa factor loadings tidak yang bernilai besar tidak lagi berkumpul pada satu faktor saja sehingga memungkinkan diinterpretasikan untuk setiap faktor yang terbentuk.

Fitur lain yang menarik untuk dibahas dalam analisis factor adalah **_Communality_**. _Communality_ menjelaskan tentang banyaknya keragamaan yang dapat dijelaskan oleh factor untuk masing-masing peubah asal. Semakin nilainya mendekati satu semakin baik keragamaan yang dapat dijelaskan.

```{r}
fa_jobApp4_rotate$communalities
```

Dalam penggunan 4 factor ini nilai _communality_ untuk semua peubah asal bernilai besar (lebih dari 0.6) sehingga dengan penggunaan 4 faktor sudah tepat.  

## Tahap 4 - Rotasi Factor (Menggunakan 2 Factor)

```{r}
fa_jobApp2_rotate <- fa(data_jobApp, nfactors = 2, fm = "minres", rotate = "varimax")
print(fa_jobApp2_rotate$loadings, cut = 0)
```

Untuk penggunaan dua factor juga akan nilai factor loadings yang besar sudah tersebar. Oleh karena itu memungkinkan diinterpretasikan. 

```{r}
fa_jobApp2_rotate$communalities
```

Dalam penggunan 2 factor ini nilai _communality_ untuk terdapat peubah asal bernilai kecil (kurang dari 0.4) sehingga dengan penggunaan 2 factor kurang tepat.

Dalam hal ini lebih baik menggunakan 4 factor saja, namun dalam ilustrasi ini 2 factor akan tetap digunakan untuk memperlihatkan perbedaan interpretasi dalam pemilihan banyaknya faktor digunakan.

| Note: |
| ---------- |
| _Communalities_ bisa diperiksa saat sebelum kita melakukan rotasi. |

## Tahap 5 - Interpretasi Factor (Menggunakan 4 Factor)

Untuk mempermudah interpretasi, factor loading yang ditampilkan selain [-0.6, 0.6].
```{r}
print(fa_jobApp4_rotate$loadings, cut = 0.6)
```

Pada penggunaan 4 factor, factor laoding hasil rotasi dapat diinterpretasikan sebagai berikut:

a. _Accademic Records_, _Experience_, _Job Fit_, dan _Potential_ memiliki nilai factor loading yang besar dan positif untuk factor 1, kita bisa menyebut factor 1 sebagai **ketepatan penempatan dan potensi berkembang bagi pegawai dalam perusahaan**.
b. _Communication_, _Company Fit_, dan _Organization_ memiliki nilai factor loading yang besar dan positif untuk factor 2 sehingga kita bisa menyebut factor 2 dengan **kemampuan dalam bekerja (_work skills_)**.  
c. _Letter_ dan _Resume_ memiliki nilai factor loading yang besar dan positif untuk factor 3 sehingga kita bisa menyebut factor 3 dengan **kemampuan menulis**.
d. _Appearance_, _Likeability_, and _Self Confidence_ memiliki nilai factor loading yang besar dan positif untuk faktor 4 sehingga kita bisa menyebut factor 4 dengan **kualitas personal pegawai**.

## Tahap 5 - Interpretasi Factor (Menggunakan 2 Factor)
```{r}
print(fa_jobApp2_rotate$loadings, cut = 0.6)
```

Pada penggunaan 2 factor, factor laoding hasil rotasi dapat diinterpretasikan sebagai berikut:

a. _Appearance_, _Communication_, _Likeability_, dan _Organization_ memiliki nilai factor loading yang besar dan positif untuk factor 1, kita bisa menyebut factor 1 sebagai **_soft-skill_ dari pegawai**. 
b. _Academic Record_, _Experience_, _Job Fit_, _Potential_, dan _Resume_ memiliki nilai factor loading yang besar dan positif untuk factor 2, kita bisa menyebut factor 2 sebagai **_hard-skill_ dari pegawai**.

Berdasarkan penggunaan 4 factor dan 2 factor diperoleh interpretasi yang berbeda dari factor-factor yang dihasilkan. Sehingga penggunaan 4 factor atau 2 factor bisa disesuaikan dengan kebutuhan perusahaan tersebut.

| Note: |
| ---------- |
| Berikut adalah contoh penerapan FA menggunakan R. Jika kalian mencoba metode pendugaan yang berbeda dan rotasi berbeda maupun jumlah factor berbeda kalian akan menemukan interpretasi yang berbeda pula. Tidak ada benar dan salah dalam interpretasi factor pada FA. |

# Pertemuan 9 Analisis Cluster

## K-Means

### Input data

Seorang pemilik Mall ingin mengelompokan customer di Mall yang ia miliki, sehingga tim marketing bisa mengembangkan strategi yang tepat untuk customer yang tepat pula. Data yang dimiliki oleh Mall tersebut adalah **Customer ID**, **umur pelanggan (_age_)**, **pendapatan tahunan dalam ribu dollar (_annual income_)**, dan **_spending score_**. Spending score merupakan nilai yang diberikan oleh Mall kepada customer berbasarkan perilaku customer (waktu kunjungan,jenis barang yang dibeli, dan banyaknya uang yang dihabiskan dalam belanja) yang memiliki rentang nilai 1-100. Semakin besar nilai Spending Score berarti customer semakin loyal pada Mall tersebut dan semakin besar pula uang belanja yang digunakan.

```{r}
library("factoextra")
data_mall <- read.csv("https://raw.githubusercontent.com/nurkhamidah/dat/main/mall_customers.csv", sep = ",")
head(data_mall)
```

## _Pre-processing_ Data

Peubah yang digunakan untuk menerapkan K-Means adalah peubah `Age AnnualIncome` dan `Spending Score` (numerik). Oleh karena itu peubah yang tidak kita gunakan akan kita hilangkan tersebih dahulu.

```{r}
data_mall <- data_mall[,c("Age","Annual.Income","Spending.Score")]
head(data_mall)
```

## Standardisasi Peubah

Standardisasi peubah merupakan proses transformasi peubah menjadi peubah yang memiliki rata-rata = 0 dan simpangan baku = 1. Proses standardisasi ini dilakukan jika kita melihat perbedaan satuan pengukuran peubah-peubah yang digunakan contoh (umur dan pendapatan). Standardisasi dilakukan karena metode K-Means menggunakan konsep jarak antara objek/amatan, yang mana sensitif terhadap satuan pengukuran. Formula untuk standardisasi data adalah sebagai berikut:

$y' = \frac{y-\bar{y}}{\sigma_y}$

Di mana:

- $\bar{y}$ : rataan $y$
- $\sigma_y$ : simpangan baku $y$

Dengan R, standardisasi dilakukan dengan fungsi `scale()`.

```{r}
data_mall_standardize <- scale(data_mall) 
```

Jika kita perhatikan rata-rata peubah setelah distandardisasi mendekati nol.
```{r}
apply(data_mall_standardize, 2, mean)
```

Dan simpangan baku dari peubah setelah distandardisasi mendekati satu.
```{r}
apply(data_mall_standardize, 2, sd)
```

| **Note** |
|----------|
| Dalam tahapan _pre-processing_ data, kita menyiapkan data agar metode K-Means bisa diterapkan secara maksimal. Dua hal yang umumnya dilakukan pada tahap ini adalah memilih peubah yang digunakan dan melakukan standarisasi peubah. |

## Memilih Banyaknya Gerombol

Umumnya, banyaknya gerombol dapat ditentukan dengan menggunakan beberapa kriteria statistik, seperti koefisien **_silhouette_** dan **WSS** atau (_Within Sum of Square_).

- Kriteria koefisien _silhouette_ dihitung berdasarkan jarak antar amatan. Koefisien ini mengukur seberapa dekat suatu amatan dengan amatan lain yang berada di gerombol yang sama (dikenal sebagai ukuran _cohesion_) dibandingkan dengan jarak terhadap amatan lain yang berada di gerombol berbeda (dikenal sebagai ukuran _separation_). Koefisien yang nilainya semakin besar menunjukkan bahwa gerombol yang terbentuk sudah sesuai.

- Kriteria WSS merupakan kriteria yang menghitung keragamaan dalam gerombol yang terbentuk. Semakin kecil keragaman dalam gerombol yang terbentuk menunjukkan bahwa gerombol yang terbentuk sudah sesuai.

Dengan menggunakan kriteria tersebut, kita bisa membandingkan banyaknya gerombol yang paling sesuai pada data yang kita sedang analisis. Dalam R, fungsi `fviz_nbclust()` dari _package_ 1factoextra1 dapat digunakan untuk memilih banyaknya gerombol.

```{r}
fviz_nbclust(data_mall_standardize, FUNcluster = kmeans, method = "silhouette")
fviz_nbclust(data_mall_standardize, FUNcluster = kmeans, method = "wss")
```

Untuk kriteria koefisien _silhouette_, banyaknya gerombol dengan **nilai koefisien tertinggi** yang kita pilih. Sedangkan pada WSS, banyaknya gerombol yang kita pilih didasarkan pada banyaknya gerombol yang mana garisnya **berbentuk seperti siku (_elbow_)**. Pada gambar diatas garis membentuk siku saat berada di gerombol keempat. **Karena penentuan ini berdasarkan visual, jadi setiap orang mungkin berbeda melihat pola sikunya.**

Berdasarkan kedua kriteria tersebut, banyaknya gerombol terbaik yang dipilih berbeda. Jika demikian, banyaknya gerombol bisa ditentukan berdasarkan kemudahan interpretasi gerombol yang terbentuk. Pada tulisan ini kita akan menggunakan 4 gerombol saja.

| **Note** |
|----------|
| Secara default banyaknya gerombol yang dicobakan pada fungsi `fviz_nbclust()` adalah 10, jika ingin merubah hal tersebut bisa dilakukan dengan menggunakan argumen `kmax` dalam fungsi, misal `kmax = 20`. |

## Menerapkan K-Means

Setelah kita mendapatkan banyaknya gerombol terbaik, maka selajutnya kita akan menerapkan metode K-Means untuk mendapatkan label gerombol pada setiap amatan. Fungsi eclust dari _package_ `factoextra` digunakan untuk menerpkan metode K-Means. Pada fungsi `eclust()`, kita cukup memasukan data yang sebelum distandardisasi, karena dalam fungsi tersebut terdapat argumen `stand`, yang jika diatur `stand = TRUE` secara otomatis data yang kita gunakan akan distandardisasi.

```{r}
kmeans_mall <- eclust(data_mall, stand = TRUE, FUNcluster = "kmeans", k=4, graph = F) 
```

Memanggil label gerombol untuk setiap amatan dilakukan dengan:
```{r}
kmeans_mall$cluster
```

Kemu- dian,interpretasi setiap gerombol yang terbentuk dapat dilakukan dengan menggunakan bantuan nilai rata-rata dari masing-masing peubah dihitung berdasarkan gerombol. Informasi ini bisa diperoleh dengan menggunakan `$centers`. Karena kita melakukan standardisasi peubah, maka nilai rata-rata yang diperoleh juga dalam skala standardisasi.
```{r}
kmeans_mall$centers
```

## Interpretasi Gerombol yang terbentuk

Berdasarkan nilai rata-rata dari `$centers`, berikut adalah interpretasinya:

- Gerombol 1 : gerombol ini merupakan customer-customer yang cukup muda (peubah **Age** bernilai kecil) dan berpenghasilan besar (peubah **Income** bernilai besar) namun sedikit sekali menghabiskan uangnya untuk berbelanja (peubah **Spending Score** bernilai kecil bahkan negatif).
- Gerombol 2 : gerombol ini merupakan customer-customer yang sudah tua (peubah **Age** bernilai besar) dan berpenghasilan kecil (peubah **Income** bernilai kecil) dan sedikit sekali menghabiskan uangnya untuk berbelanja (peubah **Spending Score** bernilai kecil). Gerombol ini mungkin murupakan customer yang sudah pensiun dan hanya memiliki pemasukan dari tunjangan pensiun.
- Gerombol 3 : gerombol ini merupakan customer-customer yang masih sangat muda (peubah **Age** bernilai kecil) dan berpenghasilan kecil (peubah **Income** bernilai kecil) namun menghabiskan uangnya untuk berbelanja cukup besar (peubah **Spending Score** bernilai besar). Gerombol ini mungkin murupakan customer yang aneh, karena memiliki penghasilan yang kecil namun belanjanya banyak.
- Gerombol 4 : gerombol ini merupakan customer-customer yang masih cukup muda (peubah **Age** bernilai kecil) dan berpenghasilan besar (peubah **Income** bernilai besar) namun menghabiskan uangnya untuk berbelanja cukup besar (peubah **Spending Score** bernilai besar). Gerombol ini mungkin murupakan customer yang paling menarik untuk menjadi target marketing selanjutnya.

Jika sulit membaca hasil dalam bentuk skala standardisasi maka kita bisa menggunakan fungsi `aggregate()` untuk melihat rata-ratanya dalam skala aslinya. Fungsi ini dapat menghitung rata-rata setiap peubah berdasarkan gerombol yang terbentuk.

```{r}
aggregate(data_mall, by =list(gerombol = kmeans_mall$cluster), FUN = mean)
fviz_cluster(kmeans_mall)
```

Cara lain untuk menginterpretasikan hasil gerombol adalah menggunakan scatterplot. Jika peubah untuk membangun cluster lebih dari dua, maka sebelum dibentuk _scatterplot_ peubah tersebut direduksi terlebih dahulu menggunakan analisis komponen utama menjadi dua komponen utama. Namun, untuk interpretasinya setiap gerombolnya kita harus mengetahui interpretasi dari kedua komponen utama dan belum tentu dengan dua komponen utama tersebut sudah mampu menjelaskan keragaman data asal dengan baik.

Interpretasi dua komponen utama bisa dilihat dengan akar cirinya.
```{r}
pca_mall <- prcomp(data_mall_standardize) 
pca_mall$rotation
```

## Profiling cluster

```{r}
# Ambil data centeroids
centers <- kmeans_mall$centers

# Konvert data centeroids ke data frame
centers_df <- as.data.frame(centers)

# Tambahkan Kolom Kluster
centers_df$cluster <- rownames(centers_df)

# Konversi Data Wide ke Long
library(tidyr)
centers_long <- gather(centers_df, key = "variable", value = "value", -cluster)
```

```{r}
# Plot the cluster profiles
library(ggplot2)
ggplot(centers_long, aes(x = cluster, y = value, fill = variable)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Cluster", y = "Mean Value", fill = "Variable") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("Profil Klaster Rata-rata Pengeluaran Perkapita Seminggu Menurut \n Kelompok Daging di Jawa Barat Menurut Kabupaten/Kota di Jawa Barat")
```

- Gerombol 1 : gerombol ini merupakan customer-customer yang cukup muda (peubah **Age** bernilai kecil) dan berpenghasilan besar (peubah **Income** bernilai besar) namun sedikit sekali menghabiskan uangnya untuk berbelanja (peubah **Spending Score** bernilai kecil bahkan negatif).
- Gerombol 2 : gerombol ini merupakan customer-customer yang sudah tua (peubah **Age** bernilai besar) dan berpenghasilan kecil (peubah **Income** bernilai kecil) dan sedikit sekali menghabiskan uangnya untuk berbelanja (peubah **Spending Score** bernilai kecil). Gerombol ini mungkin murupakan customer yang sudah pensiun dan hanya memiliki pemasukan dari tunjangan pensiun.
- Gerombol 3 : gerombol ini merupakan customer-customer yang masih sangat muda (peubah **Age** bernilai kecil) dan berpenghasilan kecil (peubah **Income** bernilai kecil) namun menghabiskan uangnya untuk berbelanja cukup besar (peubah **Spending Score** bernilai besar). Gerombol ini mungkin murupakan customer yang aneh, karena memiliki penghasilan yang kecil namun belanjanya banyak.
- Gerombol 4 : gerombol ini merupakan customer-customer yang masih cukup muda (peubah **Age** bernilai kecil) dan berpenghasilan besar (peubah **Income** bernilai besar) namun menghabiskan uangnya untuk berbelanja cukup besar (peubah **Spending Score** bernilai besar). Gerombol ini mungkin murupakan customer yang paling menarik untuk menjadi target marketing selanjutnya.

Jika sulit membaca hasil dalam bentuk skala standardisasi maka kita bisa menggunakan fungsi `aggregate()` untuk melihat rata-ratanya dalam skala aslinya. Fungsi ini dapat menghitung rata-rata setiap peubah berdasarkan gerombol yang terbentuk.

## _Hierarchical Clustering_

_Hierarchical clustering_ menghasilkan hierarki yang dapat dilihat dalam bentuk dendrogram. Dendrogram ini menunjukkan cara objek-objek tersebut dikelompokkan dalam kelompok-kelompok yang berjenjang, dan dapat dipilih sejauh mana tingkat hierarki yang ingin dieksplorasi dengan memotong dendrogram pada tingkat yang sesuai.

Dua jenis _hierarchical clustering_:

- _**Agglomerative Clustering**_ : Dalam metode _agglomerative_, proses dimulai dengan menganggap setiap objek sebagai kelompok tunggal, dan kemudian objek-objek tersebut bergabung dalam kelompok yang semakin besar berdasarkan kesamaan mereka.
- _**Divisive Clustering**_ : Dalam metode _divisive_, proses dimulai dengan menganggap semua objek sebagai satu kelompok besar, dan kemudian kelompok ini dibagi menjadi kelompok yang semakin kecil berdasarkan perbedaan antara objek-objeknya.

Pemilihan jenis _linkage_ (cara mengukur jarak antara kelompok) dalam _hierarchical clustering_ adalah keputusan penting dalam analisis gerombol ini. _Linkage_ mempengaruhi bagaimana kelompok-kelompok dibentuk dan hierarki yang dihasilkan dalam dendrogram. Beberapa jenis _linkage_ antara lain:

1. _Single linkage_ (berdasarkan jarak terdekat antara anggota kelompok)
2. _Complete linkage_ (berdasarkan jarak terjauh antara anggota kelompok)
3. _Average linkage_ (berdasarkan jarak rataan antara anggota kelompok)
4. _Ward's Method_ (berdasarkan variansi dalam anggota kelompok)
5. _Centroid Linkage_ (berdasarkan centroid antara anggota kelompok)

Tahap analisis gerombol dengan _hierarchical clustering_:
1. _Pre-processing_ data
2. Memilih metode _linkage_ dan banyaknya gerombol
3. Menerapkan _Hierarchical Clustering_
4. Interprestasi gerombol yang terbentuk

### Contoh Kasus

Contoh kasus yang digunakan adalah kasus yang sama dengan contoh sebelumnya, dengan data yang dipakai adalah data numerik yang distandardisasi sebagaimana data pada contoh sebelumnya.

```{r}
head(data_mall_standardize)
```

### Memilih Metode _Linkage_ Banyaknya Gerombol

Untuk memilih metode linkage dan banyaknya gerombol bisa menggunakan

1. Koefisien _silhoutte_ dan WSS (seperti K-Means) 
2. Menggunakan dendogram

### Menggunakan koefisien _silhouette_ dan WSS

Untuk ilustrasi kita akan menggunakan metode _silhouette_ saja karena lebih mudah menentukan jumlah gerombolnya.

```{r}
# complete
fviz_nbclust(data_mall_standardize, FUNcluster = hcut, method = "silhouette", hc_method = "complete", hc_metric = "euclidean")
# average
fviz_nbclust(data_mall_standardize, FUNcluster = hcut, method = "silhouette", hc_method = "average", hc_metric = "euclidean")
# centroid
fviz_nbclust(data_mall_standardize, FUNcluster = hcut, method = "silhouette", hc_method = "centroid", hc_metric = "euclidean")
# ward
fviz_nbclust(data_mall_standardize, FUNcluster = hcut, method = "silhouette", hc_method = "ward.D", hc_metric = "euclidean")
```

Berdasarkan koefisien _silhouette_, metode **_complete_** dan **_average_** memilih 5 gerombol, sedangkan metode **_centroid_** dan **_ward_** masing-masing memilih 2 dan 6 gerombol. Untuk saat ini, kita akan mencoba menggunakan 5 gerombol dengan metode **_complete_** (Jika dua metode _linkage_ memilih banyaknya gerombol yang sama, gerombol yang terbentuk akan relatif mirip, oleh karena itu bisa pilih salah satu).

### Menggunakan dendogram

Penggunaan dendogram untuk data yang memiliki amatan yang banyak mungkin tidak efektif karena memilih gerombol dengan dendogram dilakukan secara visual.

```{r}
linkage_methods <- c("complete", "average", "centroid", "ward.D") 
hc_mall_dend <- lapply(linkage_methods, function(i)
    hclust(dist(data_mall_standardize, method = 'euclidean'), method = i)
    )
```

```{r}
# complete
fviz_dend(hc_mall_dend[[1]])
```

```{r}
# average
fviz_dend(hc_mall_dend[[2]])
```

```{r}
# centroid
fviz_dend(hc_mall_dend[[3]])
```

```{r}
# ward
fviz_dend(hc_mall_dend[[4]])
```

Jika diperhatikan dari keempat dendogram pada masing-masing metode _linkage_, banyaknya gerombol yang terbentuk sama seperti menggunakan keofisien _silhouette_ diatas.

### Menerapkan _Hierarchical Clustering_

```{r}
hc_mall <- eclust(data_mall,stand = TRUE,FUNcluster = "hclust", k=5, hc_method = "complete", hc_metric = "euclidean")
hc_mall$cluster
```

### Interprestasi Gerombol yang terbentuk

Coba lakukan interpretasi gerombol seperti metode kmeans diatas.

```{r}
aggregate(data_mall,by =list(gerombol=hc_mall$cluster), FUN = mean)
```

```{r}
fviz_cluster(hc_mall)
```

Interpretasi dua komponen utama bisa dilihat dengan akar cirinya.

```{r}
pca_mall <- prcomp(data_mall_standardize)
pca_mall$rotation
```

# Pertemuan 10 Analisis Diskriminan

## Langkah-langkah Analisis Diskriminan

Analisis diskriminan dilakukan dengan langkah-langkah berikut:

1. Membagi data menjadi dua bagian: data latih (_train data_) dan data uji (_test data_)
    Cara untuk membagi dapat dilakukan dengan pengambilan acak. Proporsi data latih dapat sebesar 70% atau 80% dari data asal. Misal, jika memiliki data dengan 100 amatan, maka dipilih secara acak 70 atau 80 amatan sebagai data latihnya. Tujuan dari pembagian data adalah untuk mengukur dan memastikan seberapa baik model yang dibangun dalam memprediksi data yang tidak pernah dilakukan pemodelan sebelumnya. Data latih dipakai untuk membangun model, data uji dipakai untuk menguji kemampuan model yang dibangun.

2. Dengan menggunakan data latih, lakukan:
    a. Uji normal ganda
    b. Uji asumsi kesamaan ragam. Jika uji ini menghasilkan kesimpulan matriks ragam-peragam sama, maka digunakan _Linear Discriminant Analisis_ (LDA). Jika tidak, maka digunakan _Quadratic Discriminant Analisis_ (QDA).

    | Catatan: |
    |-------------|
    | Menurut Mattjik dan Sumertajaya pada buku Sidik Peubah Ganda, “umumnya sangat sulit sekali untuk dapat memenuhi persyaratan (a) dan (b), yang dalam praktek sering kali tidak diuji; hal mana akan membuat akurasi dari analisis dengan fungsi diskriminan akan berkurang. Namun demikian, fungsi diskriminan selalu menghasilkan estimasi yang kokoh (_robust estimates_) terutama yang berkaitan dengan prediksi pengelompokan”. |

    c. Estimasi koefisien analisis diskriminan
    d. Evaluasi kemampuan klasifikasi analisis diskriminan

3. Evaluasi kemampuan klasifikasi menggunakan data uji

```{r}
library(rattle)
library(caret)
library(MVN)
library(heplots)
library(MASS)
```

## Input Data

Dataset _Wine_ ini berisi hasil analisis kimia anggur yang tumbuh di daerah tertentu di Italia. Tiga jenis anggur direpresentasikan dalam 178 sampel, dengan hasil dari 13 analisis kimia yang dicatat untuk setiap sampel. Peubah `Type` telah diubah menjadi peubah kategorik.

```{r}
data(wine, package = "rattle")
head(wine)
dim(wine)
```

Diperoleh bahwa data terdiri dari 13 peubah prediktor dan 1 peubah respon sebagai berikut:

| Nama peubah | Keterangan |
|----------------|------------------|
| `Type` | Tipe anggur, terdiri dari: 1 (59 amatan), 2 (71 amatan), dan 3 (48 amatan) |
| `Alcohol` | Kadar alkohol |
| `Malic` | Kadar asam malat |
| `Ash`  | Abu |
| `Alcalinity` | Alkalinitas abu |
| `Magnesium` | Kadar Magnesium |
| `Phenols` | Total fenol |
| `Flavonoids` | Kadar fenol flavonoid |
| `Nonflavonoids` | Kadar fenol nonflavonoid |
| `Proanthocyanins` | Proantosianidin |
| `Color` | Intensitas warna |
| `Hue` | Spektrum warna |
| `Dilution` | Dilusi anggur D280/OD315 |
| `Proline` | Kadar prolin |

Menampilkan sebagian data dan tipe peubahnya sebagai berikut:
```{r}
glimpse(wine)
```

## Tahap 1 : Membagi data menjadi data latih dan data uji

Pembagian data dapat dilakukan dengan menggunakan fungsi `createDataPartition()` dari pacakge `caret`. Sintaks `caret::createDataPartition()` berarti kita memanggil fungsi `createDataPartition()` dari `caret` tanpa perlu memanggil _package_ `caret` menggunakan `library(caret)`. Argumen `y` merupakan peubah respon/gerombol, `p` merupakan proporsi data latih (dalam hal ini 0.7 atau 70%), `list=FALSE` berarti hasil output dari `createDataPartition()` disimpan dalam bentuk vektor (defaultnya `list=TRUE`)

```{r}
set.seed(123)
index_train <- caret::createDataPartition(y = wine$Type, p = 0.7, list = FALSE)
wine_train <- wine[index_train,]
wine_test <- wine[-index_train,]
```

`index_train` berisi vektor dari urutan/indeks amatan (1, 2, ..., dst) yang telah dilakukan pengambilan contoh secara acak.

## Tahap 2 : Pembangunan model dengan data latih

Dengan menggunakan data latih yang telah diambil, dilakukan langkah berikut:

### 1. Uji normalitas ganda

    H0: Data menyebar normal ganda
    H1: Data tidak menyebar normal ganda

    Untuk menguji kenormalan ganda di R, bisa menggunakan fungsi `mvn()` dari _package_ `MVN`. Fungsi `mvn` memiliki beberapa uji normal ganda yang bisa dilakukan. Pemilihan uji normal ganda bisa dilakukan melauli argumen `mvnTest`, seperti uji Mardia (`mvnTest="mardia"`), uji Henze-Zirkler (`mvnTest="hz"`), uji Royston (`mvnTest="royston"`), uji Doornik-Hansen (`mvnTest="dh"`), dan uji _energy_ (`mvnTest="energy"`). Argumen `subset` diisi dengan kolom data yang menyatakan gerombol.

    ```{r}
    uji_normalGanda <- mvn(data = wine_train, subset="Type", mvnTest = "hz")
    uji_normalGanda$multivariateNormality
    ```

    Karena nilai dari p-value dari `$1` dan `$3` adalah 0.1761976 dan 0.1661439, yang mana lebih besar dari nilai α (0.05) maka dapat disimpulkan bahwa tidak cukup bukti untuk menolak H0. Artinya, untuk peubah-peubah penjelas pada wine tipe 1 dan tipe 3 berdistribusi normal ganda. Sementera itu, _p-value_ dari `$2` sangat kecil yaitu 0.0003652993 yang mana lebih kecil dari nilai α (0.05). Artinya peubah-puebah penjelas pada wine tipe 2 tidak berdistribusi normal ganda.

    | Catatan: |
    |---------------|
    | Jika salah satu uji normal ganda menyatakan Tolak H0 maka perlu dicoba uji kenormalan yang lain, karena berpotensi hasil dari uji normal ganda lainnya menghasilkan kesimpulan yang berbeda. |
    | Walaupun ada satu gerombol yang tidak memenuhi asumsi normal ganda, analisis akan tetap dilanjutkan menggunakan analisis diskriminan karena berdasarkan Matjik dan Sumertajaya, fungsi diskriminan masih dapat menghasilkan **kemampuan klasifikasi yang baik.** |

### 2. Uji kesamaan ragam

    H0: Ragam antar populasi sama
    H1: Ragam antar populasi tidak sama

    Uji kesamaan ragam bisa dilakukan dengan menggunkan fungsi `boxM()` yang berasal dari _package_ `heplots`. Fungsi ini hanya membutuhkan 2 argumen, yaitu `data` dalam bentuk `data.frame` atau `matrix` tanpa kolom gerombol (dalam hal ini kolom **Type**) dan vektor gerombol yang diperoleh dari data (dalam hal ini kolom **Type**).

    ```{r}
    boxM(wine_train[,-1],wine_train$Type)
    ```

    Karena nilai dari _p-value_ dari uji Box’s M adalah kurang dari 2.2e-16, yang mana lebih kecil dari nilai α (0.05) maka dapat disimpulkan bahwa cukup bukti untuk menolak H0. Artinya untuk peubah-puebah penjelas pada wine memiliki **ragam yang tidak sama**. Hal ini berarti model yang lebih cocok digunakan adalah **QDA**.

    Untuk ilustrasi kita akan menggunakan LDA juga karena LDA juga berpotensi mengungguli QDA dalam hal kemampuan klasifikasi berdasarkan argumen dari Matjik dan Sumertajaya.

### 3. Estimasi koefisien analisis diskriminan

    Estimasi LDA dan QDA dapat dilakukan dengan menggunakan fungsi `lda()` dan `qda()` dari package `MASS`. Argumen minimum yang dibutuhkan oleh kedua fungsi tersebut adalah `formula` dan `data`. Argumen `formula` berisi tentang rumus model yang digunakan tanpa koefisien, `Type ~ .` berarti kolom `Type` menjadi peubah respon dan `.` menandakan memakai semua kolom kecuali kolom `Type` sebagai peubah penjelas. Argumen `data` berisi tentang data yang kita gunakan.

    ```{r}
    # LDA
    wine_lda <- lda(Type ~ ., data = wine_train)
    coef(wine_lda)
    ```

    Karena terdapat **tiga** gerombol pada data wine maka fungsi diskriminan yang terbentuk sebanyak **dua**. Secara umum fungsi diskriminan yang terbentuk dari **g** gerombol adalah **g − 1** gerombol.

    Fungsi diskriminan pertama dapat ditulis:

    $$D_1 = -0.483076356 * Alcohol + 0.224630564 * Malic - 0.672781084 * Ash 
    + 0.152999471 * Alcalinity - 0.001907630 * Magnesium + 0.705495150 * Phenols
    - 1.735333782 * Flavonoids - 0.760204649 * Nonflavonoids + 0.092670993 * Proanthocyanins 
    + 0.410568456 * Color - 0.089314921 * Hue - 0.882807209 * Dilution - 0.002645145 * Proline$$

    Fungsi diskriminan kedua dapat ditulis:

    $$D_2 = 0.854816559 * Alcohol + 0.354730236 * Malic + 2.639186194 * Ash 
    - 0.168693427 * Alcalinity - 0.004035584 * Magnesium + 0.523268740 * Phenols
    - 0.868717551 * Flavonoids - 1.589205566 * Nonflavonoids - 0.365830979 * Proanthocyanins
    + 0.277565083 * Color - 1.096511470 * Hue + 0.087609503 * Dilution + 0.002808837 * Proline$$

    ```{r}
    # QDA
    wine_qda <- qda(Type ~ ., data = wine_train)
    coef(wine_qda)
    ```

    Berbeda dengan LDA, QDA tidak memiliki koefisien yang bisa ditampilkan.

### 4. Evaluasi kemampuan klasifikasi analisis diskriminan

    Sebelum kita mengevaluasi kedua model, maka kita akan mengekstrak prediksi gerombol yang dihasilkan oleh kedua model.

    ```{r}
    predict_lda <- predict(wine_lda)
    predict_qda <- predict(wine_qda)
    ```

    - Menggunakan histogram (khusus LDA)
    ```{r}
    # fungsi diskriminan pertama
    ldahist(predict_lda$x[,1], g = wine_train$Type)
    # fungsi diskriminan kedua
    ldahist(predict_lda$x[,2], g = wine_train$Type)
    ```

    Histogram fungsi diskriminan pertama, digunakan untuk melihat kemampuan fungsi diskriminan yang pertama dalam membedakan ketiga gerombol. Karena histogram kedua dan ketiga hanya beririsan di dipinggirnya saja, maka dapat dikatakan bahwa **fungsi diskriminan pertama cukup baik untuk membedakan gerombol 2 dan gerombol 3**. Sedangkan histogram pertama dan ketiga tidak beririsan sama sekali, yang berarti **fungsi diskriminan pertama memiliki kemampuan membedakan gerombol 1 dan gerombol 3 dengan sangat baik**. Di sisi lain histogram pertama dan histogram kedua relatif besar irisanya sehingga bisa dikatakan **fungsi diskriminan pertama tidak terlalu baik dalam membedakan kelompok 1 dan kelompok 2**.

    Histogram fungsi diskriminan kedua memiliki interpretasi yang mirip seperti yang diatas. Berdasrkan histogram ini, **fungsi diskriminan kedua tidak mampu membedakan gerombol 1 dan gerombol 3**.

    - Tabel klasfikasi dan tingkat kesalahan klasifikasi

    Tabel klasifikasi bisa dimunculkan dengan menggunakan fungsi _table_, yang argumen pertamanya merupakan **gerombol asli** dan argumen keduanya merupakan **gerombol hasil prediksi**.

    ```{r}
    # LDA
    table(wine_train$Type, predict_lda$class)
    ```
    
```{r}
 # QDA
    table(wine_train$Type, predict_qda$class)
```


    Baris melambangkan gerombol asli dan kolom melambangkan gerombol hasil perdiksi. Contoh membaca tabel ini adalah sebagai berikut: misalnya saja banyaknya gerombol 1 yang terprediksi sebagai gerombol 1 juga adalah 42, banyaknya gerombol 2 yang terprediksi gerombol 3 adalah 0.
    
    Berdasarkan kedua tabel klasifikasi ini bisa dilihat bahwa LDA dan QDA tidak memiliki kesalahan dalam memprediksi ketiga gerombol tersebut. Hal ini ditunjukkan dengan hanya diagonal tabel saja yang berisi nilai.
    Tingkat kesalahan klasifikasi dihitung dengan menjumlahkan berapa banyak kesalahan prediksi gerombol yang dilakukan oleh model dibagi dengan banyaknya amatan data.

    Menghitung tingkat kesalahan klasifikasi dengan:

    ```{r}
    #LDA
    sum(wine_train$Type != predict_lda$class)/length(predict_lda$class)
    ```
    ```{r}
    #QDA
    sum(wine_train$Type != predict_qda$class)/length(predict_qda$class)
    ```

    Hasil tingkat kesalahan klasifikasi 0, berarti model LDA mampu memprediksi gerombol untuk semua amatan dengan benar. Pada output selanjutnya, dapat disimpulkan bahwa kemampuan klasifikasi LDA dan QDA sama walaupun menurut uji kesamaan ragam model yang lebih cocok adalah model QDA.

## Tahap #3 : Evaluasi kemampuan klasifikasi model dengan data uji

Pada tahap terakhir ini akan dilakukan evaluasi kemampuan klasifikasi jika seandainya terdapat data baru yang tidak terlibat dalam proses pemodelan. Sebelum mengevaluasi kedua model, maka akan dilakukan ekstraksi prediksi gerombol yang dihasilkan oleh kedua model pada data baru ini. Argumen `newdata` disi dengan data baru (data uji).

```{r}
predict_lda_test <- predict(wine_lda, newdata = wine_test)
predict_qda_test <- predict(wine_qda, newdata = wine_test)
```

Diperoleh tabel klasifikasi berikut:

```{r}
# LDA
table(wine_test$Type, predict_lda_test$class)
```
```{r}
# QDA
table(wine_test$Type,predict_qda_test$class)
```

Berdasarkan tabel klasifikasi diatas, terlihat bahwa model LDA masih memiliki kesalahan prediksi gerombol. Kesahalahan prediksi ini terjadi pada gerombol 3, dimana diprediksi gerombolnya 2.

Kesalahan klasifikasi dihitung dengan:

```{r}
#LDA
sum(wine_test$Type!=predict_lda_test$class)/length(predict_lda_test$class)
```

```{r}
#QDA
sum(wine_test$Type!=predict_qda_test$class)/length(predict_qda_test$class)
```

Berdasarkan tingkat kesalahan klasifikasi model QDA memiliki tingkat kesalahan klasifikasi yang lebih kecil sehingga dapat dikatakan model QDA lebih baik daripada model LDA. Hal ini selaras dengan model QDA yang lebih cocok digunakan sesuai dengan uji keterpenuhan asumsi yang telah dilakukan sebelumnya.

Untuk melihat amatan mana yang disalahklasifikasikan oleh LDA, dilakukan dengan melihat kelas hasil prediksi data ujinya.

```{r}
cbind(wine_test, Type_pred = predict_lda_test$class)[wine_test$Type != predict_lda_test$class,]
```

| Catatan: |
|-----------|
| Note Dalam praktiknya data baru yang dimaksud belum memiliki gerombol asli, sehingga tidak memungkinkan untuk dilakukan evaluasi. |

## Studi Kasus 2

- Sebuah Program Pascasarjana di sebuah perguruan tinggi melakukan evaluasi terhadap keberhasilan studi mahasiswa S2 di semester pertama.
- Data menunjukkan bahwa ada tiga pengelompokan status mahasiswa yang berkaitan dengan kelanjutan studi di semester 2, yaitu Lanjut ke semester 2, Pindah ke program studi lain, dan Drop Out.
- Banyak mahasiswa yang tidak dapat melanjutkan ke semester dua dengan lancar, diduga karena kurang seleksinya kualitas input mahasiswa.
- Oleh karena itu, Ketua Program Studi tersebut berencana membuat kriteria seleksi untuk menyaring mahasiswa yang berkualitas.
- Variabel yang digunakan sebagai dasar penentuan kriteria seleksi adalah IPK S1 dan skor hasil tes masuk S2. Lakukan analisis yang sesuai untuk tujuan tersebut.

### Menyiapkan Data

```{r}
data_ipk <- read.csv("https://raw.githubusercontent.com/nurkhamidah/dat/main/data-ipk.csv", sep = ";")
head(data_ipk)
```

```{r}
# merubah "Status" menjadi peubah kategorik
data_ipk$Status <- as.factor(data_ipk$Status)
glimpse(data_ipk)
summary(data_ipk)
```

Membagi data menjadi data latih dan data uji dengan proporsi 80:20 sebagai berikut:

```{r}
set.seed(1)
index_train <- caret::createDataPartition(y = data_ipk$Status, p = 0.8, list = FALSE)
ipk_train <- data_ipk[index_train,]
ipk_test <- data_ipk[-index_train,]
```

### Membangun model dengan data latih

- Uji normalitas ganda

    H0 : data menyebar normal ganda 
    H1 : data tidak menyebar normal ganda

    ```{r}
    uji_normalGanda <- mvn(data = ipk_train, subset="Status", mvnTest = "hz")
    uji_normalGanda$multivariateNormality
    ```

    Diperoleh bahwa dari ketiga kelas (Lanjut, Drop out, dan Pindah), ketiganya menyebar normal ganda terlihat dari nilai _p-value_ yang diperoleh sudah > alfa 5%. Asumsi normalitas ganda terpenuhi.

- Uji kesamaan ragam

    H0 : ragam antar populasi sama 
    H1 : ragam antar populasi tidak sama

    ```{r}
    boxM(ipk_train[,-3], ipk_train$Status)
    ```
    
    Karena _p-value_ pengujian lebih kecil dari alfa 5%, maka dapat disimpulkan bahwa cukup bukti untuk menolak H0. Artinya untuk peubah-puebah penjelas pada status akademik memiliki ragam yang tidak sama sehingga metode yang lebih cocok adalah QDA. Namun karena _p-value_ mendekati 5%, maka dicoba dilakukan analisis dengan LDA dan QDA. 

- Estimasi koefisien diskriminan

    ```{r}
    # LDA
    ipk_lda <- lda(Status ~ .,data = ipk_train)
    coef(ipk_lda)
    ```

    Diperoleh dua fungsi diskriminan berikut:

    $$D_1 = -4.806460162 * IPK - 0.008604346 * Skor.tes$$

    $$D_2 = -1.9347744 * IPK + 0.0140985 * Skor.tes$$

    ```{r}
    # QDA
    ipk_qda <- qda(Status ~ .,data = ipk_train)
    coef(ipk_qda)
    ```

- Evaluasi model LDA dengan data latih dengan histogram

    ```{r}
    predict_lda2 <- predict(ipk_lda)
    predict_qda2 <- predict(ipk_qda)
    ```

    ```{r}
    # fungsi pertama
    ldahist(predict_lda2$x[,1], g = ipk_train$Status)
    # fungsi kedua
    ldahist(predict_lda2$x[,2], g = ipk_train$Status)
    ```

    Diperoleh dari histogram fungsi diskriminan pertama, bahwa terdapat irisan di pinggirnya pada kelompok _Drop Out_ dan _Pindah_, yang artinya fungsi diskriminan pertama **cukup baik** dalam mengklasifikasikan kelompok _Drop Out_ dan _Pindah_. Sementara, pada kelompok _Lanjut_ dan _Drop Out_, tidak terdapat irisan sama sekali pada histogramnya yang artinya fungsi diskriminan pertama **sangat baik** dalam mengklasifikasikan kelompok mahasiswa yang _Drop Out_ dan _Lanjut_. Terdapat irisan yang cukup banyak antara histogram kelompok _Lanjut_ dan _Pindah_, yang artinya fungsi diskriminan pertama **tidak cukup baik* dalam mengklasifikasikan kelompok mahasiswa yang _Lanjut_ atau _Pindah_.

    Selanjutnya pada fungsi diskriminan kedua, diperoleh bahwa fungsi diskriminan kedua belum cukup baik untuk mengklasifikasikan ketiga status mahasiswa terlihat dari irisannya yang besar.

- Evaluasi model LDA  & QDA dengan data latih menggunakan histogram

    ```{r}
    # LDA
    table(ipk_train$Status, predict_lda2$class)
    # tingkat kesalahan klasifikasi
    sum(ipk_train$Status != predict_lda2$class)/length(predict_lda2$class)
    ```

    Berdasarkan data latih, diperoleh bahwa terdapat satu mahasiswa yang _Drop Out_ namun diklasifikasikan _Pindah_ berdasarkan model LDA, serta dua mahasiswa yang _Lanjut_ namun diklasifikasikan _Pindah_. Selain itu, tidak ada mahasiswa yang _Drop Out_ namun salah diklasifikasikan _Lanjut_ pada model diskriminan ini, karena terlihat dari fungsi diskriminan pertamanya yang sangat baik dalam mengklasifikasikan dua kelas tersebut. Selanjutnya, dari tiga mahasiswa yang salah diklasifikasikan, diperoleh bahwa tingkat kesalahan klasifikasi oleh model LDA pada data latih ini sebesar 4.35%.

    ```{r}
    # QDA
    table(ipk_train$Status, predict_qda2$class)
    # tingkat kesalahan klasifikasi
    sum(ipk_train$Status != predict_qda2$class)/length(predict_qda2$class)
    ```

    Berdasarkan data latih pula, diperoleh bahwa terdapat tiga mahasiswa yang salah diklasifikasikan sehingga besaran tingkat kesalahan klasifikasinya juga sama besar. Di mana, masing-masing satu mahasiswa diklasifikasikan salah sebagai berikut: mahasiswa _Pindah_ diklasifikasikan _Lanjut_, dan sebaliknya, serta mahasiswa _Drop Out_ diklasifikasikan _Pindah_.

#### Evaluasi model dengan data uji

```{r}
predict_lda_test2 <- predict(ipk_lda, newdata = ipk_test)
predict_qda_test2 <- predict(ipk_qda, newdata = ipk_test)
```

Berdasarkan data uji, diperoleh bahwa terdapat satu mahasiswa yang _Drop Out_ namun diklasifikasikan _Pindah_ berdasarkan model LDA, serta dua mahasiswa yang _Lanjut_ namun diklasifikasikan _Pindah_. Selain itu, tidak ada mahasiswa yang _Drop Out_ namun salah diklasifikasikan _Lanjut_ pada model diskriminan ini, karena terlihat dari fungsi diskriminan pertamanya yang sangat baik dalam mengklasifikasikan dua kelas tersebut. Selanjutnya, dari tiga mahasiswa yang salah diklasifikasikan, diperoleh bahwa tingkat kesalahan klasifikasi oleh model LDA pada data uji ini sebesar 18.75%.

Sedangkan berdasarkan data uji pula, diperoleh tidak ada yang salah diklasifikasikan dengan model QDA. Terlihat bahwa model QDA lebih baik dalam memprediksi kelas/status akademik mahasiswa.

Untuk melihat amatan yang salah diprediksi, dilakukan dengan code sebagai berikut:

```{r}
# LDA
table(ipk_test$Status, predict_lda_test2$class)
# tingkat kesalahan klasifikasi
sum(ipk_test$Status != predict_lda_test2$class)/length(predict_lda_test2$class)
# melihat amatan yang salah diklasifikasikan
cbind(ipk_test, Status_pred = predict_lda_test2$class)[ipk_test$Status != predict_lda_test2$class,]
```

```{r}
# QDA
table(ipk_test$Status, predict_qda_test2$class)
# tingkat kesalahan klasifikasi
sum(ipk_test$Status != predict_qda_test2$class)/length(predict_qda_test2$class)
```

# Pertemuan 11 Analisis Biplot

## Langkah-Langkah

Langkah-langkah melakukan analisis biplot secara manual adalah sebagai berikut:

1. Siapkan data dalam bentuk matriks **X**, di mana matriks **X** adalah matriks data yang telah dikoreksi terhadap rata-ratanya
2. Hitung **X'X** kemudian cari akar cirinya, $\lambda_i$, dan vektor ciri yang bersesuaian
3. Hitung matriks **U**, **L**, dan **A**
4. Cari nilai **G** dan **H**
5. Tentukan G* (kolom ke-1 dan ke-2 dari matriks **G**) sebagai koordinat objek dan H* (kolom ke-1 dan ke-2 matriks **H**) sebagai koordinat peubah
6. Gambar biplot dengan koordinat G* dan H*
7. Hitung ukuran kebaikan biplot (besar keragaman yang dapat dijelaskan oleh biplot yang terbentuk)

## Studi Kasus 1

Tabel berikut memberikan data beberapa  universitas dengan beberapa peubah yang digunakan untuk perbandingan/perangkingan universitas, di mana terdiri dari:

- `SAT` : rata-rata nilai SAT mahasiswa baru
- `Top10` : persentase mahasiswa baru yang merupakan 10% teratas di SMA asal
- `Accept` : persentase pelamar yang diterima
- `SFRatio` : rasio fakultas dan mahasiswa
- `Expenses` : biaya pendidikan tahunan (estimasi), dan
- `Grad` : rasio kelulusan (%)

### Menyiapkan data

```{r}
univs <- read.table(header = T, text = 
"
University;SAT;Top10;Accept;SFRatio;Expenses;Grad
Harvard;14;91;14;11;39.525;97
Princeton;13.75;91;14;8;30.22;95
Yale;13.75;95;19;11;43.514;96
Stanford;13.6;90;20;12;36.45;93
MIT;13.8;94;30;10;34.87;91
Duke;13.15;90;30;12;31.585;95
Dartmouth;13.4;89;23;10;32.162;95
", sep=";")
head(univs)
```

Agar nama-nama objek (universitas) menjadi nama bagi masing-masing baris, maka dilakukan sesuai kode berikut:

```{r}
rownames(univs) <- univs$University
univs_new <- univs[,-1] #hapus kolom nama universitas
head(univs_new)
```

##3 Melakukan eksplorasi data

Diperoleh ringkasan/deskripsi data sebagai berikut.

```{r}
summary(univs_new)
```

Dari hasil di atas, diperoleh statistik 5 serangkai dari masing-masing peubah yang menjelaskan universitas.

Kemudian dilakukan visualisasi data menggunakan boxplot sebagai berikut:

```{r}
boxplot(univs_new)
```

Boxplot digunakan untuk memeriksa keberadaan pencilan dalam suatu peubah. Terlihat dari plot di atas, bahwa peubah `Grad` memiliki pencilan bawah. Sedangkan 5 peubah lainnya tidak memiliki pencilan.
Selanjutnya, dilakukan eksplorasi mengenai korelasi antar peubah dengan fungsi `corrplot()`.

```{r}
library(corrplot)
corrplot(cor(univs_new), method = "number", is.cor = T, type = "lower", diag=F)
```

Diperoleh bahwa peubah `Top10` berkorelasi secara positif dengan `SAT` dan `Expenses` Sementara itu, peubah `Top10` berkorelasi negatif dengan peubah `Accept` dan `SFRatio`. Artinya, semakin besar proporsi mahasiswa yang merupakan 10% teratas di sekolah untuk masuk ke universitas tersebut, semakin kecil presentase penerimaannya dan semakin besar presentase kelulusan dan biaya pendidikannya. Dan sebagainya.

### Analisis biplot

Analisis ini menggunakan beberapa _package_ antara lain:

- `FactoMineR` untuk membentuk fungsi komponen utama
- `factoextra` untuk membentuk grafik biplot

```{r}
library(FactoMineR)
library(factoextra)
```

1. Penskalaan/pengoreksian data

Sebelum melakukan analisis biplot, dilakukan pengoreksian data (matriks **X**) terhadap rataannya menggunakan fungsi `scale()`.

```{r}
df = scale(univs_new,scale = FALSE)
head(df)
```

2. Pembentukan fungsi komponen utama dan grafik biplot

Pembentukan fungsi komponen utama dilakukan dengan menggunakan fungsi `PCA()`

```{r}
df_pca = PCA(df, graph = F,scale.unit = F)
```

Kemudian dilakukan pembentukan grafik biplot dengan fungsi `fviz_pca_biplot()` yang membutuhkan argumen berupa fungsi komponen utama yang telah dibentuk.

```{r}
fviz_pca_biplot(df_pca,repel = T) + theme_classic()
```

Dari grafik di atas, diperoleh informasi sebagai berikut:

1. **Keragaman peubah**
    - Peubah `Expenses` dan `Accept` memiliki keragaman yang lebih besar dibandingkan peubah lainnya.
    - Peubah `SAT` memiliki keragaman terkecil.
2. **Keeratan hubungan antar peubah**
    - Peubah `Grad`, `SAT` dan `Expenses` memiliki korelasi yang positif terlihat dari sudut yang terbentuk membentuk sudut < 90 derajat.
    - Peubah `Top10` dan `SFRatio` serta `SFRatio` dan `Accept` juga memiliki korelasi yang positif terlihat sudutnya < 90 derajat. 
    - Korelasi negatif dimiliki oleh hubungan peubah `Top10` dan `Grad` serta `Top10`, dan `Accept`.
3. **Kemiripan antar objek** dan **Posisi relatif objek dengan peubah penciri**
    - MIT dan Duke digambarkan dalam posisi yang berdekatan, artinya keduanya memiliki karakteristik yang mirip.
    - Yale dicirikan dengan peubah `Expenses` yang bernilai cukup tinggi.
    - Princeton memiliki `SFRatio` di bawah rata-rata.  
    
Silahkan lanjutkan dengan interpetasi yang lain.  

Selanjutnya untuk mengevaluasi kebaikan dari biplot yang terbentuk, dilakukan dengan melihat persentase kumulatif keragaman yang diperoleh dari dua nilai eigen teratas (karena biplot yang terbentuk dua dimensi).

Kebaikan biplot dilihat berdasarkan `cumulative.variance.percent` dari nilai eigen kedua. (Informasi keragaman yang mampu dijelaskan oleh biplot adalah sebesar 92.28%. Artinya biplot mampu memberikan informasi dari 6 peubah untuk menilai posisi relatif 7 Universitas)

```{r}
eigen_values <- get_eigenvalue(df_pca)
eigen_values
```

Selain itu, dapat melihat presentase keragaman biplot dengan grafik plot scree dengan menggunakan `fviz_eig()` berikut.
```{r}
fviz_eig(df_pca, addlabels = T) + theme_classic()
```

## Studi Kasus 2

Pertengahan tahun 2003 yang lalu Bank ”BR” melakukan evaluasi terhadap keuntungan yang mereka peroleh. Sebagai bank pemerintah dengan aset yang cukup besar, BR tidak memiliki keuntungan yang cukup tinggi dan bahkan cenderung mengalami penurunan keuntungan akibat menurunnya kuantitas dan kualitas nasabahnya. Managemen BR merekomendasikan pelaksanaan riset pasar terhadap persepsi masyarakat terhadap performa BR dan perbandinganya dengan performa bank-bank nasional lainnya. Dengan mengetahui posisi bank berdasarkan persepsi masyarakat ini, diharapkan dapat ditempuh langkah-langkah yang lebih baik.

Riset dilakukan dalam bentuk survei untuk mengumpulkan data persepsi masyarakat terhadap beberapa bank nasional. Survei tersebut melibatkan 2300 responden yang tersebar di 8 kota (Jakarta, Surabaya, Medan, Makasar, Balikpapan, Denpasar, Solo, dan Palembang). Setiap responden diminta untuk memberikan skor terhadap atribut/karakteristik berikut:

- Kualitas petugas bank dalam memberikan layanan 
- Kualitas produk yang ditawarkan oleh bank 
- Kualitas fasilitas kantor cabang
- Kualitas dan kuantitas fasilitas ATM
- Kualitas hadiah yang ditawarkan oleh bank 
- Keamanan bertransaksi
- Kemudahan mencapai lokasi pelayanan bank

Penilaian dilakukan menggunakan skala 1, 2, ..., 10. Nilai yang semakin besar menunjukkan kecenderungan yang positif terhadap atribut yang ditanyakan.

### Menyiapkan data

```{r}
bank <- read.table(header = T, text = 
"
layanan;produk;cabang;ATM;hadiah;aman;lokasi
4.31;3.89;5.67;5.89;5.12;7.11;4.12
7.64;7.4;8.58;8.15;6.74;8.23;6.77
8.94;7.12;8.61;6.89;7.45;8.35;7.89
7.21;4.2;6.39;3.58;4.23;6.87;4.34
5.74;6.34;8.22;6.31;7.05;7.65;6.89
4.03;3.55;4.26;4.09;6.9;6.8;3.44
6.96;3.11;4.09;3.23;3.21;6.12;2.86
4.99;5;6.31;6.17;5.89;7.62;5.36
",
sep = ";")
rownames(bank) <- c("BR", "BC", "BMD", "NI", "BN", "BDN", "BMG", "BPM")
bank
```

### Analisis biplot

```{r}
bank2 = scale(bank,scale = FALSE)
bank_pca = PCA(bank2, graph = F, scale.unit=F)
fviz_pca_biplot(bank_pca,repel = T) + theme_classic()
```

- Dari grafik di atas, diperoleh bahwa peubah `layanan` memiliki keragaman terbesar dan peubah `aman` memiliki keragaman terkecil.
- Semakin sempit sudut yang dibuat antara dua peubah maka semakin positif tinggi korelasinya, seperti: `cabang`, `produk` dan `lokasi`; memiliki korelasi positif yang cukup tinggi.
- Sedangkan jika sudutnya tumpul (berlawanan arah) maka korelasinya negatif, seperti: `hadiah` dan `layanan`.
- Semakin dekat letak objek dengan arah yang ditunjuk oleh suatu peubah maka semakin tinggi nilai peubah tersebut untuk objek itu. Sedangkan jika arahnya berlawanan, maka nilainya rendah.
- **BC** dan **BMD** merupakan bank yang memiliki tingkat `layanan`, `cabang`, `produk`, dan `lokasi` yang tinggi.
- **BN** memiliki tingkat `keamanan`, `ATM`, dan `hadiah` yang tinggi.
- Semakin dekat letak dua buah objek maka sifat yang ditunjukkan oleh nilai-nilai peubahnya semakin mirip.
- **BC** dan **BMD** memiliki indikator yang hampir mirip.
- **BMG** dan **NI** juga membentuk kelompok sendiri.
- **BDN** dan **BR** juga membentuk kelompok sendiri berdasarkan kemiripan sifat peubah.  

(Interpretasi lain dapat ditambahkan sendiri)  

Kemudian dilakukan evaluasi hasil biplot yang terbentuk sebagai berikut:

```{r}
eigen_values2 <- get_eigenvalue(bank_pca)
eigen_values2
```

```{r}
fviz_eig(bank_pca, addlabels = T) + theme_classic()
```

Informasi keragaman yang mampu dijelaskan oleh biplot adalah sebesar 93.57%. Artinya biplot mampu memberikan informasi dari 7 peubah untuk menilai posisi relatif 8 bank berdasarkan kualitasnya.

# Pertemuan 12 Analisis Korespodensi

## Langkah-Langkah

Beberapa langkah yang dilakukan untuk membentuk plot korespondensi antara lain:

1. Siapkan data, kemudian bentuk tabel kontingensi.
2. Berdasarkan tabel kontingensi, bentuk tabel korespondensi dengan cara membagi masing-masing nilai dalam tabel kontingensi dengan n (banyak amatan).
3. Bentuk vektor baris, vektor kolom, matriks baris, dan matriks kolom berdasarkan tabel korespondensi yang telah dibuat.
4. Lakukan analisis profil baris dengan membagi proporsi/frekuensi masing-masing elemen dengan total proporsi/frekuensi masing-masing baris.
5. Lakukan analisis profil kolom dengan membagi proporsi/frekuensi masing-masing elemen dengan total proporsi/frekuensi masing-masing kolom.
6. Tentukan koordinat profil baris dan kolom dengan menggunakan Generalized SVD.
7. Bentuk plot korespondensi berdasarkan koordinat yang diperoleh.

## Studi Kasus 1 - Data Stasiun TV

- Suatu survei dilakukan untuk mengetahui stasiun TV favorit menurut pemirsa dengan kelompok usia tertentu.
- Ada 5 stasiun TV yang menjadi pilihan di dalam survei, yaitu **MetroTV**, **Indosiar**, **NETTV**, **TransTV**, dan **RCTI**.
- Sementara respondennya, dikelompokkan dalam 4 kelompok umur, yaitu **> 50 th**, **40-50 th**, **20-39 th**, dan **10-19 th**.

### Import Data

```{r}
stasiun_tv <- read.csv("https://raw.githubusercontent.com/nurkhamidah/dat/main/stasiun_tv.csv", sep = ";")
stasiun_tv
```

### Penyelesaian Manual

1. **Tabel Kontingensi**

    Dari data yang terbentuk, dapat kita bentuk tabel kontingensi sebagai berikut:

    ```{r}
    stasiun_tv$Usia <- factor(stasiun_tv$Usia, levels=c("> 50 th", "40-50 th",
                                                        "20-39 th", "10-19 th"))
    stasiun_tv$Stasiun.TV <- factor(stasiun_tv$Stasiun.TV,
                                    levels=c("MetroTV", "Indosiar",
                                             "NETTV","TransTV","RCTI"))
    table_count <- xtabs(Jumlah ~ Usia + Stasiun.TV, data = stasiun_tv)
    table_count
    ```

2. **Tabel Korespondensi**

    ```{r}
    n <- sum(table_count)
    table_coresp <- table_count/n
    table_coresp
    ```

3. **Vektor kolom dan vektor baris**

    Berdasarkan matriks korespondensi yang terbentuk, diperoleh vektor kolom dan vektor baris berikut:

    ```{r}
    c <- colSums(table_coresp)
    c
    ```

    ```{r}
    r <- rowSums(table_coresp)
    r
    ```

4. **Matriks kolom dan matriks baris**

    ```{r}
    Dc <- diag(c)
    Dc
    ```

    ```{r}
    Dr <- diag(r)
    Dr
    ```

5. **Analisis profil baris**

    Profil baris diperoleh dengan:

    $$R = Dr^{-1}P$$

    ```{r}
    R <- solve(Dr)%*%table_coresp
    rownames(R) <- rownames(table_coresp)
    R
    ```

    - Pada kolom MetroTV, dapat dilihat bahwa baris usia >50 tahun mempunyai nilai tertinggi (0.454). Hal ini menunjukkan bahwa stasiun METRO TV menjadi stasiun TV favorit pemirsa yang berusia > 50 tahun.
    - Pada kolom Indosiar, dapat dilihat bahwa pemirsa dengan rentang usia 40-50 tahun memiliki nilai tertinggi (0.0734). Hal ini menunjukkan bahwa stasiun Indosiar menjadi stasiun TV favorit pemirsa yang berusia 40- 50 tahun.
    - Pada kolom NETTV, dapat dilihat bahwa baris usia 20-39 tahun memiliki nilai tertingggi (0.5124). Hal ini menunjukkan bahwa statiun NETTV menjadi stasiun TV favorit untuk pemirsa pada rentang usia 20-39 tahun.
    - Pada kolom TransTV dan RCTI, dapat dilihat bahwa usia 10-19 tahun memiliki nilai tertinggi (0.0646 dan 0.5179). Hal ini menunjukkan bahwa kedua stasiun televisi ini menjadi stasiun TV favorit pemirsa pada rentang usia 10-19 tahun.

    Kemudian diperoleh nilai massa setiap kolom sebagai berikut:

    ```{r}
    mass_c <- colSums(R)
    mass_c
    ```

    Nilai massa terbesar adalah 1.52414 terdapat pada kolom stasiun televisi NETTV, masih sama dengan modus amatan yang kita peroleh pada data awal.

6. **Analisis profil kolom**

    Profil baris diperoleh dengan:

    $$C = P Dc^{-1}$$

    ```{r}
    C <- table_coresp %*% solve(Dc)
    colnames(C) <- colnames(table_coresp)
    C
    ```

    - Usia pada kategori > 50 tahun dan 40-50 tahun, mempunyai massa terbesar pada stasiun televisi MetroTV yaitu 0.2240 dan 0.4729. Hal ini menunjukkan bahwa usia terbanyak yang sering menonton METRO TV adalah pemirsa pada usia > 50 tahun dan 40-50 tahun.
    - Usia pada kategori 20-39 tahun, mempunyai massa terbesar pada stasiun televisi NETTV yaitu 0.4254. Hal ini menunjukkan bahwa usia terbanyak yang sering menonton NET TV adalah pemirsa pada usia 20-39 tahun.
    - Usia pada kategori 10-19 tahun, mempunyai massa terbesar pada stasiun televisi RCTI yaitu 0.7203. Hal ini menunjukkan bahwa usia terbanyak yaang sering menonton RCTI adalah pemirsa pada usia 10-19 tahun.

    Kemudian diperoleh nilai massa setiap baris sebagai berikut:

    ```{r}
    mass_r <- rowSums(C)
    mass_r
    ```

    Nilai massa terbesar adalah 1.633683 terdapat pada usia kategori 10-19 tahun, jadi berbeda dengan modus amatan yang diperoleh pada data awal yaitu pada kategori usia 20-39 tahun.

7. **Menentukan koordinat profil baris dan kolom**

    Koordinat baris dan kolom yang ditentukan menggunakan GSVD melalui matriks:

    $$P-rc'$$

    ```{r}
    Prc <- table_coresp - r%*%t(c)
    Prc
    ```

    Kemudian matriks Z diperoleh melalui:

    ```{r}
    Z <- diag(1/sqrt(diag(Dr)))%*%Prc%*%diag(1/sqrt(diag(Dc)))
    Z
    ```

    ```{r}
    Du <- diag(sqrt(eigen(Z %*% t(Z))$values))[1:2,1:2]
    Du
    ```

    Kemudian dari matriks **Z** yang sudah diperoleh, dicari matriks **U** (vektor ciri dari **ZZ'**) dan matriks **V** (vektor ciri dari matriks **Z'Z**).

    ```{r}
    library(factoextra)
    U <- eigen(Z %*% t(Z))$vectors
    U
    A <- (sqrt(Dr) %*% U)[,1:2]
    A
    ```

    ```{r}
    V <- eigen(t(Z) %*% Z)$vectors
    V
    B <- (sqrt(Dc) %*% V)[,1:2]
    B
    ```

    **Koordinat baris**
    ```{r}
    rows <- solve(Dr)%*%A%*%Du
    rows
    ```

    **Koordinat kolom**
    ```{r}
    cols <- solve(Dc)%*%B%*%Du
    cols
    ```

8. **visualisasi berdasarkan koordinat profil baris dan kolom**

```{r}
row_df <- data.frame(rows)
col_df <- data.frame(cols)
colnames(row_df) <- c("Dim.1", "Dim.2")
rownames(row_df) <- rownames(table_count)
colnames(col_df) <- c("Dim.1", "Dim.2")
rownames(col_df) <- colnames(table_count)
row_df["Var"] <- "Usia"
row_df["Size"] <- 2
col_df["Var"] <- "Stasiun TV"
col_df["Size"] <- 2
ca.plot.df <- rbind(col_df, row_df)
ca.plot.df["Label"] <- rownames(ca.plot.df)
ca.plot.df
```

```{r}
library(ggplot2)
p <- ggplot(ca.plot.df, aes(x = Dim.1, y = Dim.2,
                       col = Var, shape = Var,
                       label = Label, size = Size)) +
  geom_vline(xintercept = 0, lty = "dashed", alpha = .5) +
  geom_hline(yintercept = 0, lty = "dashed", alpha = .5) +
  geom_point() + geom_text(check_overlap = T, hjust=-0.15)

plot(p)
```

### Penyelesaian dengan R

Untuk menyelesaikan analisis korespondensi dengan R, data yang digunakan adalah data yang telah direpresentasikan pada tabel kontingensi, sebagaimana yang tersimpan pada objek `table_count`. _Package_ yang digunakan adalah `FactoMineR` dengan fungsi `CA()`, serta visualisasi menggunakan `factoextra`.

```{r}
library(factoextra)
library(FactoMineR)
table_count
```

### Eksplorasi Data

Coba kita lakukan visualisasi dari tabel kontingensi yang terbentuk.

```{r}
# install.packages("gplots") # run ini jika package belum terinstall
library(gplots)
balloonplot(table_count, main ="Tabel Kontingensi Stasiun TV x Usia", xlab ="", ylab="", label = FALSE, show.margins = FALSE)
```

Di mana semakin besar lingkaran menunjukkan semakin besar pula nilai yang direpresentasikan.

```{r}
chisq <- chisq.test(table_count)
chisq
```

Diperoleh dari hasil uji Khi-kuadrat, bahwa kedua peubah (stasiun TV dan rentang usia) secara signifikan saling terkait. 

### Membentuk model _Correspondence Analysis_

```{r}
model_tv <- CA(table_count, graph = F)
```

Untuk melihat objek/informasi apa saja yang terdapat dalam model, lakukan:

```{r}
print(model_tv)
```

Dengan menggunakan _package_ `factoextra`, kita dapat memperoleh banyak informasi melalui fungsi-fungsi berikut:

- `get_eigenvalue(model)`, mengekstraksi nilai akar ciri.
- `fviz_screeplot(model)`, melakukan visualisasi presentase keragaman yang diperoleh.
- `get_ca_row(model)`, `get_ca_col(model)`, melakukan analisis profil baris dan kolom.
- `fviz_ca_row(model)` `fviz_ca_col(model)`, melakukan visualisasi profil baris dan kolom.
- `fviz_ca_biplot(model)` untuk melakukan visualisasi keduanya.  

  
1. **Akar/vektor ciri**

```{r}
eig.val <- get_eigenvalue(model_tv)
eig.val
```

2. **Presentase keragaman**

```{r}
fviz_screeplot(model_tv, addlabels = TRUE)
```
  
Dapat dilihat bahwa untuk mereduksi menjadi 2 dimensi saja, kita sudah mendapatkan 99.63% keragaman dalam data.

3. **Analisis dan visualisasi profil baris**

```{r}
row <- get_ca_row(model_tv)
row
```

- Koordinat dari profil baris 

```{r}
row$coord
```

Untuk memetakan menjadi dua dimensi, maka hanya perlu melihat _Dim 1_ dan _Dim 2_ nya saja.

- Kontribusi keragaman profil baris

```{r}
row$contrib
```

Profil baris yang berkontribusi paling banyak kepada _Dim 1_ dan _Dim 2_ adalah baris yang paling penting dalam menjelaskan keragaman data. Dalam hal ini, responden berusia 10-19 tahun dan 20-39 tahun dianggap sebagai responden yang menyumbang keragaman paling besar.

- Plot profil baris

```{r}
fviz_ca_row(model_tv, col.row="steelblue", shape.row = 15)
```

-- Baris dengan profil serupa dikelompokkan bersama.  
-- Baris yang berkorelasi negatif diposisikan pada sisi yang saling berlawanan (kuadran berlawanan).  
` `
  
4. **Analisis dan visualisasi profil kolom**

```{r}
col <- get_ca_col(model_tv)
col
```

- Koordinat dari profil kolom 

```{r}
col$coord
```

Untuk memetakan menjadi dua dimensi, maka hanya perlu melihat _Dim 1_ dan _Dim 2_ nya saja.

- Kontribusi keragaman profil kolom

```{r}
col$contrib
```

Profil kolom yang berkontribusi paling banyak kepada _Dim 1_ dan _Dim 2_ adalah baris yang paling penting dalam menjelaskan keragaman data. Dalam hal ini, stasiun TV TransTV dan NETTV dianggap sebagai stasiun TV yang menyumbang keragaman paling besar.

- Plot profil kolom

```{r}
fviz_ca_col(model_tv, col.col="steelblue", shape.col = 15)
```

5. **Pembentukan plot korespondensi**

```{r}
fviz_ca_biplot(model_tv, repel = TRUE)
```

- Plot dalam gambar diatas, menunjukkan bagaimana korelasi antara stasiun TV favorit dengan berbagai kategori usia.
- Terlihat bahwa usia kategori 10-19 tahun berdekatan dengan stasiun televisi RCTI dan TransTV, artinya jelas bahwa pemirsa dengan rentang usia 10-19 tahun lebih sering menonton stasiun RCTI dan TransTV dibanding stasiun televisi lainnya.
- Kemudian pemirsa dengan kategori usia 20-39 tahun lebih sering menonton stasiun televisi NETTV.
- Terakhir, pemirsa dengan kategori usia 40-50 tahun dan > 50 tahun lebih sering menonton stasiun televisi Indosiar dan MetroTV dibanding stasiun televisi lainnya.

## Studi Kasus 2 - Data Kenyamanan Kerja x Pendapatan

Suatu data frekuensi tentang hubungan antara kenyamanan kerja dengan pendapatan yang disajikan pada tabel berikut :

1. **Import Data**
```{r}
df_kerja <- read.csv("https://raw.githubusercontent.com/nurkhamidah/dat/main/kenyamanan_pendapatan.csv", sep = ";")
rownames(df_kerja) <- df_kerja$X
df_kerja <- df_kerja[,-1]
df_kerja
```

2. **Eksplorasi dengan uji Khi-kuadrat**

H0 : variabel baris dan kolom dari tabel kontingensi adalah independen.
H1 : variabel baris dan kolom dependen.

```{r}
chisq.test(df_kerja)
```

Karena diperoleh _p-value_ < 2.2e-16 maka H0 ditolak artinya terdapat hubungan yang erat antara pendapatan dan kenyamanan pekerjaan.

3. **Analisis korespondensi**

`library(ca)` adalah _package_ R untuk mengaktifkan fungsi analisis korespondensi, jika belum tersedia `install.packages(ca)`

```{r}
# install.packages("ca")
library(ca)
fit <- ca(df_kerja)
summary(fit)
```

`fit=ca(data)` digunakan untuk melakukan analisis korespondensi. Diperoleh total keragaman yang dapat dijelaskan adalah sebesar 100%.

4. **Analisis profil baris dan kolom**
```{r}
n <- sum(df_kerja)
sum_row <- apply(df_kerja, 1, sum)
sum_row/n
```

```{r}
rows <- df_kerja/sum_row
rows
```

```{r}
sum_col <- apply(df_kerja, 2, sum)
sum_col/n
```

```{r}
cols <- df_kerja/sum_col
cols
```

5. **Pembentukan plot korespondensi**

```{r}
fit
```

```{r}
plot(fit)
```

- Pekerja yang merasa Sangat tidak nyaman dan kurang nyaman dengan pekerjaannya cenderung yang memiliki pendapatan < 25juta.
- Pendapatan diantara 25 juta hingga 50 juta cenderung merasa biasa saja terhadap pekerjaannya.
- Pendapatan yang lebih dari 50 juta merasa Nyaman dengan pekerjaannya.

# Pertemuan 13 Analisis korelasi kanonik

## Langkah-Langkah

Analisis korelasi kanonik dilakukan dengan tahapan sebagai berikut:

1. Melakukan eksplorasi data dan uji asumsi analisis korelasi kanonik
2. Menentukan fungsi kanonik dan ukuran kesesuaiannya
3. Menginterpretasikan hasil analisis korelasi kanonik
4. Melakukan validasi hasil analisis korelasi kanonik

## Studi Kasus - 1

- Suatu survey terhadap 50 orang dilakukan untuk mengetahui apa faktor yang berhubungan dengan _sales performance_.
- Peubah yang di ukur adalah _**Sales Performance (X)**_ yang terdiri atas **_Sales Growth_ (X1)**, **_Sales Profitability_ (X2)**, dan **_New Account Sales_ (X3)** serta **_Intelligence Test Score_ (Y)** yang terdiri atas **_Creativity_ (Y1)**, **_Mechanical Reasoning_ (Y2)**, **_Abstract Reasoning_ (Y3)**, dan **_Mathematics_ (Y4)**.
- Lakukan analisis dari data berikut menggunakan korelasi kanonik menggunakan R atau software lain yang sesuai.

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r}
data_sales <- read.csv("https://online.stat.psu.edu/stat505/sites/stat505/files/lesson13/sales.csv", sep = ",")
library(knitr)
kable(head(data_sales))
```

Pisahkan masing-masing peubah berdasarkan gugusnya.

```{r}
sales_X <- data_sales[,1:3]
sales_Y <- data_sales[,4:7]
```

### Uji asumsi data

```{r}
# install.packages("GGally")
library(GGally)
ggpairs(sales_X)
```

Berdasarkan plot berpasangan, terlihat bahwa terjadi multikolinearitas antar peubah X. Namun, karena berdasarkan koefisien korelasi yang dihasilkan diperoleh multikolinearitas yang tidak sempurna (|korelasi| < 1), maka dalam contoh ini kita lanjutkan saja dan diabaikan dulu.

```{r}
ggpairs(sales_Y)
```

Berdasarkan plot di atas, diperoleh bahwa tidak terjadi multikolinearitas antar peubah Y.

```{r}
ggpairs(data_sales)
```

Uji normalitas ganda dilakukan dengan:

```{r}
# install.packages("RVAideMemoire")
library(RVAideMemoire)
mqqnorm(sales_X, main = "Multi-normal Q-Q Plot X")
```

Berdasarkan QQ-plot normalitas ganda peubah X, diperoleh titik-titik menyebar di sekitar garis diagonal sehingga disimpulkan bahwa gugus peubah X menyebar secara normal ganda.

```{r}
mqqnorm(sales_Y, main = "Multi-normal Q-Q Plot Y")
```

Berdasarkan QQ-plot normalitas ganda peubah Y, terdapat titik-titik yang tidak menyebar di sekitar garis diagonal sehingga disimpulkan bahwa gugus peubah Y belum menyebar secara normal ganda.

```{r}
mqqnorm(cbind(sales_X, sales_Y), main = "Multi-normal Q-Q Plot X dan Y")
```
Berdasarkan QQ-plot normalitas ganda peubah X dan Y, diperoleh bahwa titik-titik menyebar di sekitar garis diagonal sehingga disimpulkan bahwa gugus peubah X dan Y secara bersama-sama sudah menyebar secara normal ganda.  
### Korelasi antar peubah

Periksa korelasi antar **X** dan **Y** :

```{r}
# install.packages("CCA")
library(CCA)
correls_XY <- matcor(sales_X, sales_Y)
```

Korelasi antar **X** :

```{r}
kable(correls_XY$Xcor)

library(corrplot)
corrplot(correls_XY$Xcor, method = "color", addCoef.col ='white', is.cor = T, type = "lower", diag = F)
```

- Besarnya korelasi antara X1 dan X2, X1 dan X3, serta X2 dan X3 secara berurutan adalah 0.9261, 0.8840, dan 0.8425.
- Hasil ini menunjukkan bahwa ada hubungan linear yang erat antar anggota dalam gugus peubah Sales Performance (X) karena mendekati 1 dan arahnya positif.

Korelasi antar **Y** :

```{r}
kable(correls_XY$Ycor)
corrplot(correls_XY$Ycor, method = "color", addCoef.col ='black', is.cor = T, type = "lower", diag = F)
```

- Korelasi antar anggota dalam gugus peubah Y cenderung lebih kecil dibandingkan besaran korelasi anggota dalam gugus peubah X.
- Semua nilai korelasi nya kurang dari 0.6.
- Korelasi tertinggi dimiliki hubungan peubah Creativity (Y1) dan Mechanical Reasoning (Y2) sebesar 0.59 dan terendah sebesar 0.1469 yang merupakan hubungan antar Creativity (Y1) dan Abstract Reasoning (Y3).

Korelasi antar X dan Y :

```{r}
kable(correls_XY$XYcor)
corrplot(correls_XY$XYcor, method = "color", addCoef.col ='white', is.cor = T, type = "lower", diag = F)
```

- Hasil di atas memperlihatkan bahwa Sales Growth (X1), Sales Profitability (X2) dan New Acount Sales (X3) memiliki hubungan linear yang sangat kuat dengan mathematics (Y4).
- Hubungan yang cukup kuat juga terlihat pada hubungan peubah X (X1,X2,X3) dengan mechanical reasoning (Y2).
- Sedangkan tingkat hubungan linear antar peubah X dengan creativity (Y1) dan abstract reasoning (Y3) relatif rendah.

```{r}
img.matcor(correls_XY, type = 2)
```

### Korelasi kanonik

```{r}
# install.packages("candisc")
library(candisc)
cca1 <- cancor(sales_X, sales_Y)
summary(cca1)
```

atau dapat ditemukan dengan menggunakan _package_ `CCA`:

```{r}
library(CCA)
cc1 <- cc(sales_X, sales_Y)
cc1$cor
```

- Korelasi kanonik pertama (korelasi antara pasangan pertama dari kanonik) adalah sebesar 0.994483.
- Nilai ini merepresentasikan korelasi tertinggi yang mungkin terjadi antara beberapa kombinasi linear dari _sales performance_ dan beberapa kombinasi linear dari _Intelligence Test Score_.
- Proporsi keragaman menunjukkan baik tidaknya peubah kanonik yang dipilih untuk menerangkan keragaman asal.
- Semakin besar nilai proporsi keragaman maka semakin baik peubah-peubah kanonik yang dipilih menerangkan keragaman asal. Proporsi keragaman untuk memilih jumlah peubah kanonik yang tepat digunakan untuk menjelaskan hubungan antara gugus peubah dependen dengan independen.
- Berdasarkan kontribusi keragaman yang dijelaskan oleh fungsi kanonik terlihat fungsi kanonik pertama menjelaskan kergaman total sebesar 96.21%, sedangkan yang kedua 3,61%, dan ketiga 0,18%.
- Berdasarkan proprosi keragaman diatas, dengan menggunakan kriteria batasan minimal memiliki kontribusi keragaman 70% (Mattjik dan Sumertajaya 2011), maka cukup mengambil fungsi kanonik yang pertama saja.
- Korelasi kanonik yang pertama dan kedua berbeda nyata dengan nol pada taraf nyata 1% (Pr < α, dengan α = 0.01).
- Sedangkan korelasi kanonik yang ketiga tidak berbeda nyata dengan nol pada taraf nyata 1%.
- Hal ini berarti korelasi kanonik yang dapat digunakan untuk menjelaskan hubungan antar gugus peubah _Sales Performance_ dengan _Intelligence Test Score_ adalah dua korelasi kanonik yang pertama.

### Koefisien korelasi kanonik mentah

Koefisien kanonik _raw_ / mentah untuk peubah _sales performance_:

```{r}
cc1[3:4]
```

Interpretasi koefisien di atas mirip dengan interpretasi koefisien regresi, di mana:

- Peningkatan nilai Sales Growth (X1) akan menurunkan variansi kanonik pertama dari gugus peubah _sales performance_ sebesar 0.0624 dengan peubah lainnya dianggap konstan.
- Peningkatan nilai Profitability (X2) akan menurunkan variansi kanonik pertama dari gugus peubah _sales performance_ sebesar 0.0209 dengan peubah lainnya dianggap konstan.
- dan seterusnya.

### Menghitung _loading kanonik_

Fungsi `comput()` digunakan untuk menghitung loading kanonik, yang digunakan untuk menampilkan korelasi peubah dengan fungsi kanonik sebagai berikut:

```{r}
cc1_comp <- comput(sales_X, sales_Y, cc1)
```

- Korelasi X dengan fungsi kanonik X:

```{r}
cc1_comp$corr.X.xscores
corrplot(cc1_comp$corr.X.xscores, method = "color", addCoef.col ='white', is.cor = T)
```

Diperoleh bahwa untuk fungsi kanonik peubah _sales performance_ peubah-peubah yang berhubungan paling erat dengan fungsi kanonik pertama adalah Sales Growth (X1), sedangkan terhadap fungsi kanonik kedua yaitu Sales Profitability (X2).

- Korelasi Y dengan fungsi kanonik Y:

```{r}
cc1_comp$corr.Y.yscores
corrplot(cc1_comp$corr.Y.yscores, method = "color", addCoef.col ='white', is.cor = T)
```

Untuk fungsi kanonik peubah _Intelligence Test Score_ terlihat bahwa peubah-peubah yang berhubungan paling erat dengan fungsi kanonik pertama yaitu Mathematic (Y4) kemudian Mechanical Reasoning (Y2), sedangkan terhadap fungsi kanonik kedua yaitu Abstract Reasoning (Y3).

- Korelasi silang X dengan fungsi kanonik Y:

```{r}
cc1_comp$corr.X.yscores
corrplot(cc1_comp$corr.X.yscores, method = "color", addCoef.col ='white', is.cor = T)
```

Korelasi silang antar peubah-peubah dependen terhadap fungsi kanonik peubah independen memberikah hasil bahwa yang berhubungan paling erat dengan fungsi kanonik pertama adalah Sales Growth (X1), dan dengan fungsi kanonik kedua adalah Sales Profitability (X2).

- Korelasi silang Y dengan fungsi kanonik X:

```{r}
cc1_comp$corr.Y.xscores
corrplot(cc1_comp$corr.Y.xscores, method = "color", addCoef.col ='white', is.cor = T)
```

Sedangkan korelasi silang antar peubah-peubah independen terhadap fungsi kanonik peubah dependen memberikah hasil bahwa yang berhubungan paling erat dengan fungsi kanonik pertama adalah Mathematic (Y4), dan dengan fungsi kanonik kedua adalah Abstract Reasoning (Y3).

### Uji signifikansi korelasi kanonik

```{r}
rho <- cc1$cor
n <- dim(sales_X)[1]
p <- length(sales_X)
q <- length(sales_Y)
```

```{r}
# install.packages("CCP")
library(CCP)
p.asym(rho, n, p, q, tstat = "Wilks")
p.asym(rho, n, p, q, tstat = "Hotelling")
p.asym(rho, n, p, q, tstat = "Pillai")
p.asym(rho, n, p, q, tstat = "Roy")
```

Secara umum, banyak fungsi/dimensi kanonik sama dengan jumlah peubah dalam himpunan yang lebih kecil, namun banyak dimensi yang signifikan mungkin lebih kecil lagi. Dimensi kanonik, juga dikenal sebagai fungsi kanonik, adalah peubah laten yang mirip dengan faktor-faktor yang diperoleh dalam analisis faktor. Untuk model khusus ini terdapat tiga dimensi kanonik.

Dari hasil di atas diperoleh bahwa keempat pengujian menunjukkan bahwa pada fungsi/dimensi kanonik pertama, semuanya signifikan pada taraf nyata 5%. Artinya, minimal ada satu korelasi kanonik yang nyata. Selanjutnya dilanjutkan dengan uji parsial.

```{r}
cca1
```

Berdasarkan LR test, diperoleh bahwa korelasi kanonik yang pertama dan kedua berbeda nyata dengan nol pada taraf nyata 1% (Pr < α, dengan α=0.01).
Sedangkan korelasi kanonik yang ketiga tidak berbeda nyata dengan nol pada taraf nyata 1%.
Hal ini berarti korelasi kanonik yang dapat digunakan untuk menjelaskan hubungan antar gugus peubah _Sales Performance_ dengan _Intelligence Test Score_ adalah dua korelasi kanonik yang pertama.

### Menghitung koefisien korelasi kanonik terstandardisasi

```{r}
coef_X <- diag(sqrt(diag(cov(sales_X))))
coef_X %*% cc1$xcoef
```

- Koefisien kanonik untuk fungsi kanonik dependen (_sales performance_) menunjukkan bahwa urutan kontribusi relatif peubah-peubah _sales performance_ (X) terhadap variate/fungsi kanonik pertama adalah Sales Growth (X1), New Account Sales (X3), dan Sales Profitability (X2);
- Urutan kontribusi relatif peubah-peubah X terhadap variate kedua adalah Sales Profitability (X2), Sales Growth (X1), dan New Account Sales (X3).

```{r}
coef_Y <- diag(sqrt(diag(cov(sales_Y))))
coef_Y %*% cc1$ycoef
```

- Urutan kontribusi relatif peubah-peubah _Intelligence Test Score_ (Y) terhadap variate pertama adalah Mathematic (Y4), Creativity (Y1), Abstract Reasoning (Y3), dan Mechanical Reasoning (Y2);
- Urutan kontribusi relatif peubah-peubah Y terhadap variate kedua adalah Abstract Reasoning (Y3), Creativity (Y1), Mathematic (Y4), dan Mechanical Reasoning (Y2).

### Peubah Kanonik

Diperoleh beberapa fungsi kanonik berikut:

$U_1 = -0.4577 X_1 - 0.2119 X_2 - 0.3679 X_3$  
$U_2 = -1.2772 X_1 + 2.4517 X_2 - 1.229 X_3$

$V_1 = -0.2755 Y_1 - 0.1040 Y_2 - 0.1916 Y_3 - 0.6621 Y_4$  
$V_2 = -0.7600 Y_1 + 0.6823 Y_2 - 1.0607 Y_3 + 0.7199 Y_4$


## Studi Kasus - 2

Seorang peneliti ingin melakukan suatu eksperimen yang melibatkan suatu reaksi kimia
dengan 3 peubah independen dan 3 peubah dependen (Box dan Youle, 1955; Rencher, 2002).
Peubah-peubah tersebut adalah sebagai berikut :  
X1 : temperature  
X2 : concentration  
X3 : time  
Y1 : percentage of unchanged starting material  
Y2 : percentage converted to the desired product  
Y3 : percentage of unwanted by-product  

Lakukan analisis korelasi kanonik terhadap data berikut:  

### Import data

```{r}
# install.packages("ACSWR")
library(ACSWR)
data(chemicaldata)
kable(head(chemicaldata))
```

```{r}
X <- chemicaldata[,4:6]
Y <- chemicaldata[,1:3]
```

### Periksa asumsi

```{r}
ggpairs(X)
```

```{r}
ggpairs(Y)
```

Sebagaimana plot yang diperoleh, pada gugus peubah Y terdapat peubah Y1 dan Y3 yang berkorelasi kuat dan berpotensi menyebabkan multikolinearitas.

```{r}
correl <- matcor(X, Y)
```

Berikut merupakan ilai korelasi pearson antar peubah X, antar peubah Y, dan korelasi silang antara peubah X dengan peubah Y.

```{r}
corrplot(correl$Xcor, method = "color", addCoef.col ='black', is.cor = T, type = "lower", diag = F)
```

```{r}
corrplot(correl$Ycor, method = "color", addCoef.col ='white', is.cor = T, type = "lower", diag = F)
```

```{r}
corrplot(correl$XYcor, method = "color", addCoef.col ='black', is.cor = T, type = "lower", diag = F)
```

### Korelasi Kanonik

```{r}
cca2 <- candisc::cancor(X,Y)
summary(cca2)
```

Kontribusi korelasi kanonik terbesar ditunjukkan pada fungsi kanonik pertama. Artinya untuk menerangkan keragaman total cukup mengambil fungsi kanonik pertama saja.

Uji hipotesis menunjukkan bahwa korelasi kanonik pertama berbeda nyata. Artinya korelasi kanonik yang dapat digunakan untuk menjelaskan hubungan antar gugus peubah X dan Y adalah satu korelasi kanonik.

```{r}
coef_X <- diag(sqrt(diag(cov(X))))
coef_X %*% cca2$coef$X
```

```{r}
coef_Y <- diag(sqrt(diag(cov(Y))))
coef_Y %*% cca2$coef$Y
```

Diperoleh fungsi kanonik berikut:

$U = -0.9988 X_1 - 0.6296 X_2 - 0.3522 X_3$  
$V = 1.7019 Y_1 + 0.3259 Y_2 + 0.5775 Y_3$

### Korelasi dengan fungsi kanonik

#### Korelasi X dan fungsi kanonik X (U):

```{r}
corrplot(cca2$structure$X.xscores, method = "color", addCoef.col ='black', is.cor = T, diag = F)
```


#### Korelasi Y dan fungsi kanonik Y (V):

```{r}
corrplot(cca2$structure$Y.yscores, method = "color", addCoef.col ='black', is.cor = T, diag = F)
```

#### Korelasi X dan V :

```{r}
corrplot(cca2$structure$X.yscores, method = "color", addCoef.col ='black', is.cor = T, diag = F)
```

#### Korelasi Y dan U:

```{r}
corrplot(cca2$structure$Y.xscores, method = "color", addCoef.col ='black', is.cor = T, diag = F)
```

# Pertemuan 14 Analisis Multidemensional scaling

## Metode MDS

1. **Metric / Classical Multidimensional Scaling**

    - Skala yang digunakan dalam multidimensional scaling metrik adalah skala data interval dan skala data rasio.
    - Dalam prosedur multidimensional scaling metrik hanya menyusun bentuk geometri dari titik-titik objek yang diupayakan sedekat mungkin dengan input jarak yang diberikan.
    - Pada dasarnya, MDS metrik adalah mengubah input jarak kedalam bentuk geometrik sebagai outputnya.
    - Perhitungan jarak dapat dilakukan menggunakan jarak euclid.

2. **Nonmetric Multidimensional Scaling**

    - Multidimesional scaling nonmetrik mengasumsikan bahwa datanya adalah kualitatif (nominal dan ordinal)
    - Mengasumsikan bahwa hanya peringkat jarak antar data yang diketahui
    - Teknik ini lebih fleksibel dan robust dibandingkan MDS Metrik karena dapat digunakan dengan data yang mungkin tidak memenuhi asumsi-asumsi ketat yang diperlukan oleh MDS. 
    - Dapat digunakan untuk memvisualisasikan hubungan antar objek yang tidak mudah dilihat pada data aslinya.

### Studi Kasus 1

Diberikan suatu data yang mencantumkan jarak terbang antar sepuluh kota di Amerika Serikat, seperti yang terlihat dalam data berikut.

Data ini dapat dengan mudah dibuat dari peta Amerika Serikat dengan menggunakan penggaris untuk mengukur jarak antar kota-kota tersebut.
Namun, terdapat situasi di mana seseorang diberikan hanya data jarak antar kota tersebut dan kemudian diminta untuk membuat peta berdasarkan jarak-jarak ini. Membuat peta berdasarkan data jarak ini merupakan masalah yang lebih sulit. Peta yang akurat harus memperhitungkan hubungan spasial yang sebenarnya antara kota-kota tersebut.

Multidimensional Scaling (MDS) diharapkan dapat menangani masalah ini. MDS adalah teknik analisis multivariat yang bertujuan untuk mereduksi dimensi dari data dengan tetap mempertahankan hubungan atau jarak antar objek.

1. **Menyiapkan data**

Karena data berskala kuantitatif, maka dalam kasus ini dilakukan penskalaan dengan MMDS.

```{r}
airline_data <- matrix(ncol=10,nrow=10)

colnames(airline_data) <- c("Atlanta","Chicago","Denver","Houston","Los_Angeles","Miami","New_York","San_Francisco","Seattle","Washington_D.C")

rownames(airline_data) <- c("Atlanta","Chicago","Denver","Houston","Los_Angeles","Miami","New_York","San_Francisco","Seattle","Washington_D.C")

airline_data[lower.tri(airline_data)] <- c(587,1212,701,1936,604,748,2139,2182,543,920,940,1745,1188,713,1858,1737,597,879,831,1726,1631,949,1021,1494,1374,968,1420,1645,1891,1220,2339,2451,347,959,2300,1092,2594,2734,923,2571,2408,205,678,2442,2329)

diag(airline_data) <- 0
airline_data <- as.dist(airline_data, diag = TRUE) # membuat data matriks menjadi kelas dist
class(airline_data)
airline_data
```

2. **Menghitung koordinat MMDS**

Untuk membentuk koordinat MMDS dengan fungsi `cmdscale()`. Fungsi ini memiliki beberapa argumen:

1. `d` : objek data berformat _dist_ (matriks jarak/distance)
2. `k` : maksimum dimensi di mana data akan direpresentasikan, bernilai antara 1 sampai p-1 untuk data berdimensi p
3. `eig` : argumen bertipe boolean untuk menentukan apakah nilai ciri akan dikembalikan

```{r}
mmds_coord <- cmdscale(airline_data, k = 2, eig = T)
coords <- mmds_coord$points
coords
```

3. **Membentuk plot MMDS**

```{r}
plot(coords[,1], coords[,2], type = "n", xlab = "", ylab = "", axes = FALSE,
     main = "US Airline Map - MMDS")
text(coords[,1], coords[,2], labels(airline_data), cex = 0.9, xpd = TRUE)
```

Grafik hasil menunjukkan bahwa Washington dan New York adalah kota yang berdekatan. Kedua, kita dapat melihat bahwa Los Angeles dan San Francisco berdekatan satu sama lain. Di sisi lain, Miami dan Seattle adalah kota yang paling jauh satu sama lain. Juga, San Francisco terletak jauh dari Miami, New York, dan Washington.

4. **Evaluasi kebaikan MMDS**

```{r}
#install.packages("seqhandbook")
library(seqhandbook)
stress <- seqmds.stress(airline_data, mmds_coord)
stress
plot(stress, type='l', xlab='number of dimensions', ylab='stress')
```

Diperoleh bahwa dengan memetakan ke dalam 2 dimensi, diperoleh nilai stress sebesar 0.00327 (mendekati nol), artinya penskalaan ini sudah baik.

### Studi Kasus 2

Matriks data yang digunakan dalam analisis ini mewakili jumlah perbedaan yang ada di antara para politisi Perang Dunia II.

1. **Menyiapkan data**

```{r}
ww_data <- matrix(ncol=12,nrow=12)

colnames(ww_data) <- c("Hitler","Mussolini","Churchill","Eisenhower","Stalin","Attlee","Franco","De_Gaulle","Mao_Tse","Truman","Chamberlain","Tiro")

rownames(ww_data) <- c("Hitler","Mussolini","Churchill","Eisenhower","Stalin","Attlee","Franco","De_Gaulle","Mao_Tse","Truman","Chamberlain","Tiro")

ww_data[lower.tri(ww_data)] <- c(5,11,15,8,17,5,10,16,17,12,16,14,16,13,18,3,11,18,18,14,17,7,11,11,12,5,16,8,10,8,16,16,14,8,17,6,7,12,15,13,11,12,14,16,12,16,12,16,12,9,13,9,17,16,10,12,13,9,11,7,12,17,10,9,11,15)

diag(ww_data) <- 0

class(ww_data)
ww_data
```

Karena data merupakan data kategorik, maka penskalaan dilakukan dengan metode NMMDS. Selanjutnya data diubah menjadi matriks jarak:

```{r}
ww_data <- as.dist(ww_data, diag = TRUE)
```

2. **Membentuk koordinat NMMDS**

NMMDS dilakukan menggunakan fungsi `isoMDS()` dari package `MASS` yang membutuhkan beberapa argumen:

- `d` : objek matriks jarak
- `k` : banyak dimensi yang diinginkan, defaultnya 2
- `tol` : batas toleransi iterasi sampai konvergen, secara default $10^{-3}$
- `maxit` : banyak iterasi maksimum, secara default 50

```{r}
library(MASS)
coord_nmmds = isoMDS(ww_data, k = 2)
```

3. **Membentuk plot NMMDS**

```{r}
plot(coord_nmmds$points, type = "n", xlab = "", ylab = "", axes = FALSE,)
text(coord_nmmds$points, labels(ww_data), cex = 0.9, xpd = TRUE)
```

Berdasarkan plot di atas kita dapat melihat bahwa “Franco” dan “Mussolini” memiliki kemiripan yang paling banyak dibandingkan dengan yang lain. Jelas bahwa “Hitler” dan “Mussolini” memiliki kesamaan tertinggi kedua. Selanjutnya kita dapat menemukan adanya kesamaan antara “De Gaulle” dan “Churchill”. Sebaliknya “Mussolini” dan “Attlee” memiliki jumlah ketidaksamaan tertinggi berdasarkan plot di atas.

4. **Menghitung nilai STRESS**

```{r}
stress2 <- seqmds.stress(ww_data, coord_nmmds)
stress2
plot(stress2, type='l', xlab='number of dimensions', ylab='stress')
```

Diperoleh dengan pemetaan dua dimensi, nilai stress yang didapatkan sebesar 0.218188.

```{r}
# install.packages("vegan")
library(vegan)
stressplot(coord_nmmds, ww_data)
```

Plot Shepard di atas adalah scatter plot dari jarak antar titik (ada $\frac{n(n-1)}{2}$) vs. ketidaksamaan pada amatan. Yang diharapkan adalah terdapat linieritas antara meningkatnya nilai jarak dengan _dissimilarity_, dan berdasarkan plot di atas diperoleh bahwa terdapat hubungan linier antar keduanya, artinya pemetaan yang dihasilkan sudah baik.